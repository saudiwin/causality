---
title: 'Not Causal or Descriptive But Some Secret, Other Thing: Entropy as a Criterion for Causal Learning'
date: "December 23rd, 2022"
author: 
  - "Robert Kubinec":
      email: rmk7@nyu.edu
      institute: nyuad
      correspondence: true
institute:
  - nyuad: Social Science Division, New York University Abu Dhabi, Abu Dhabi, United Arab Emirates
abstract: "While many classify studies as either descriptive or causal, I argue that causality is a continuous construct, and different inference modes--experimental, observational and mechanistic--can at best provide only partial causal information. To discriminate between the relative value of different inference modes, I employ statistical entropy as a possible yardstick for evaluating research designs as different operations on causal graphs. Rather than dichotomize studies as either causal or descriptive, the concept of entropy instead emphasizes the relative causal knowledge gained from a given research finding. I employ this theory to clarify why and when researchers relied on divergent modes of inference to determine the efficacy of vaccines over the course of the COVID-19 pandemic and to establish the causal relationship between a country's oil wealth and its predisposition towards authoritarianism. I include R code for estimating the entropy of causal graphs in the appendix.^[A reproducible version of this paper with code is available at https://github.com/saudiwin/causality/blob/master/gold_standard_covid.Rmd . I thank David Waldner, Christopher Winship, Michael Poznansky, Kevin Munger, Arthur Spirling, and Andrew Gelman for helpful comments on this manuscript.]"
bibliography: references.bib
toc: false
fontsize: 12pt
fontfamily: times
output: 
  bookdown::pdf_document2:
    keep_tex: true
    includes:
      in_header: 
        preamble2.tex
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning=FALSE,
                      message = FALSE,
                      fig.align = 'center')

# to install quickDAG, use *this* command to get correct version:


require(dplyr)
library(tidyr)
require(ggplot2)
require(ggthemes)
require(quickdag)
require(entropy)
require(network)
require(ggnetwork)
require(dagitty)
library(ggdag)
require(DiagrammeR)
require(rmutil)
require(jmuOutlier)
```

\newpage

<!-- The renewed attention to causal identification in the last twenty years has elevated the status of the randomized experiment to the sine qua non gold standard of the social sciences. Nonetheless, research employing observational data, both qualitative and quantitative, continues unabated, albeit with a new wrinkle: because observational data cannot, by definition, assign cases to receive causal treatments, any conclusions from these studies must be descriptive, rather than causal. However, even though this new norm has received widespread adoption in the social sciences, the way that social scientists discuss and interpret their data often departs from this clean-cut approach. Analyses with observational data continue to be performed in a way that suggests causal interpretations, and even qualitative evidence is cited as valid explanation for human actions. This disjuncture between the clean logic of counterfactual causal inference and actual practice has created considerable confusion among researchers who must decide whether to devote precious resources to observational data collection that is fatally doomed regardless of its promise or a (quasi-)experiment even if that experiment would not answer their research question. -->

<!--# This paper inverts the meaning of gold standard with respect to causality by considering the origin of the phrase in heated monetary debates of the 19th century. The quotation above by William Jennings Bryan referenced devaluation occurring in the United States as a consequence of pegging the dollar to gold reserves. When the price of gold increased, the money supply constricted and borrowers would have to pay more than they initially agreed to. Bryan and his confederates wanted to switch to the silver standard, of which there was a much more plentiful supply, in order to maximize much-needed specie for the United States' quickly growing economy.   -->

While the causal inference revolution has opened up vast new areas for empirical research, it has also faced criticism for pressuring researchers to produce causally-identified findings even when their data do not permit such conclusions. The underlying problem, I argue, is that scholars employ a binary classification for studies as either causal or descriptive, with the former seen as more valuable than the latter. Binary decision problems concerning p-values have been rigorously criticized in recent years for obscuring uncertainty in statistical results [@amrhein2019], and I apply the same thinking to our decisions about whether we should consider research findings causal or descriptive. Re-conceptualizing causality as a latent scale, of which different possible research designs are observable indicators, can shift the conversation away from causal versus descriptive labels and towards some secret, other thing: the relative contribution of a study in terms of its reduction of causal entropy.

<!-- The difficulty in relying solely on potential outcomes and randomization for causal inference is not that experiments can have serious flaws, although they do, but rather that scientists have yet to come up with a single empirically verifiable definition of causal relations. Causality is a latent concept; we can define with reference to examples, but the word's connotations are difficult to capture simply and clearly. Part of the problem is that causality does not refer to an observable physical process but rather to an existential frame within which human beings determine the processes that are and are not meaningful. For this reason, the social sciences are struggling not only with the estimation of uncertainty in methods, research design and the amount of data, but with knowing what it is exactly that we aim to accomplish with all this endeavor. As a result, if we assume that the beneficial properties of certain statistical paradigms make them the definition of causality, we commit a subtle logical fallacy. -->

To do so, I will make the claim in this paper that *causal identification* is not in fact necessary for causal learning. This statement is not to disparage existing theorems about identification, nor to challenge the utility of the literature for research. Rather, causal identification is a sufficient condition for causal learning, but it is not, in fact, necessary. It is unlikely that we would be able to answer all causal queries without strict causal identification, but it is not implausible, and has occurred in recent areas of research, as I explain in this article.

I argue instead that there are three main ways that scholars can obtain causal knowledge: the counterfactual mode of inference (which I also link to the closely-related manipulationist mode), the Humean (correlational) mode of inference, and the mechanistic mode of inference. Each of these modes can be seen as a kind of *silver* standard, incorporating much of what we mean by the term causal, but failing at the same time to encompass all of what we mean (if we could somehow express it).

I propose that a helpful, possibly unifying, *continuous* criterion for evaluating studies across these different modes is that of reducing entropy. Entropy is a framework widely used in statistics to represent the relative amount of information present in a random variable [@shannon1948]. Entropy is a characteristic of a random variable's distribution; the flatter (more uncertain) the distribution, the more entropy exists. When applied to causal graphs [@pearl2000], entropy can help describe how diverse types of research can yield varying amounts of causal knowledge. In this paper I use entropy as a criterion by which to evaluate the relative utility of divergent research designs without having to resort to binary classifications about causal versus exploratory research.

I show that applying entropy metrics to causal graphs returns intuitive results that provide a foundation for considering the value of different research designs over the same causal process. In a given causal graph, it is entirely possible that an observational analysis, which can only show associations, will reduce the entropy of a causal graph as much as, or more than, an experiment that is causally identified. This result closely matches many researchers' intuitions, but has not to my knowledge been previously formalized.

I show how this framework can be applied through case studies of two important cross-disciplinary research topics: the efficacy of COVID-19 vaccines and the relationship between a country's oil wealth and its level of democracy. I show through the case studies how observational, experimental and mechanistic (qualitative) research designs can each contribute causal knowledge in varying amounts depending on the particular causal structure. I also provide R code in the appendix to help researchers perform similar analyses of their own potential research designs.

# The New Intellectual Battlefield: Causality

The credibility revolution of the past fifteen (or so) years, which argues for the application of the potential outcomes framework and RCTs as a way of measuring the credibility of statistical analyses, has produced a sea change in how political scientists, economists and increasingly others measure research success. The potential outcome theories behind the credibility revolution date back to the 1970s or even earlier [@fisher1935; @rubin1974; @holland1986], but for whatever reason, the practice of formal randomized experiments did not take off in fields besides psychology until the 2000s [@green2003; @morgan2007; @levitt2008].

More recently, a second credibility revolution has swept through social scientific disciplines that long employed experiments, particularly psychology. This second revolution has questioned the use of discretized decision rules, i.e. p-values, as a source of inferring causal inference, and shown that many published experiments fail to replicate even if the original experiment had statistically significant results [@opensciencecollaboration2015; @gelman2013]. This revolution has emphasized pre-registering research questions [@nosek2018], sharing data so that conclusions can be replicated or reproduced [@goodman2016], and even more radical changes, such as fundamentally altering the standards used to judge statistical inference [@benjamin2017; @amrhein2019]. While the first revolution has dramatically elevated the status of experimental research designs, the second has ironically pointed to deep problems in how RCTs have been implemented and evaluated.

As a result, there remain significant pockets of resentment at the success of the potential outcomes framework. The success of experiments appears to endanger the role of observational studies, whether qualitative or quantitative, as these studies can never meet the stringent criteria imposed by their randomized kin [@beck2006; @gerber2014; @gerstein2019]. Previously popular methods like large-N time-series cross section models have come under criticism for failing to either estimate *average treatment effects* (ATEs) [@samii2016; @imbens2018; @gibbons2017]; the causal criterion of the potential outcomes framework, or to account for missing variables and over-time dynamics [@plümper2019]. To enforce the distinction, journals increasingly require scholars to avoid "causal" language like "impact" or "effect" when using observational methods [@hernán2018; @thapa2020; @yu2019].

All of this is not to say that scholars have not already worked to define a more accomodating relationship between causality and description. After analyzing a difficult inference problem relating to breast cancer research, @imai2008 argued that "[n]either [experimental nor observational design] is better; both are adapted as well as possible to the constraints of their subjects and research situation" (p. 493). More recently, @ashworth2021 argue that theorists and empirical scholars need to understand the overlap between their approaches to avoid belittling research questions that have theoretical significance yet lack strict causal identification (and vice versa).

The most relevant paper to the causal-description binary, though, is @gerring2012a definition of "mere" description as "aim[ing] to answer *what* questions ... about a phenomenon or set of phenomena" (p. 722). By providing a definition for descriptive research, Gerring argued for thinking of description as an "independent" yet necessary task in the social and physical sciences. Gerring points out that the downstream consequence of failing to value descriptive research rests on research evaluation: "the fact remains that the descriptively oriented researcher faces a higher hurdle in the race to publication than the causally oriented researcher" (p. 741). In this paper, I take Gerring's ideas, one might say, to their logical conclusion: research activities given the label "descriptive" are of equal merit to causal inference because they are in fact different modes of causal learning. While descriptive types of research can be given an inductive definition, as Gerring does admirably well, it is still difficult to properly value "mere" description when it is considered ontologically separate from causal inference.

The main problem, I maintain, underlying these disagreements is an acute problem for social scientists: while we all want to obtain causal knowledge, we do not in fact know exactly what causal knowledge is. Existing research shows that causal thinking is deeply connected to human thought processes [@sloman2015]. Causality involves assigning meaning to events, an endeavor that in fact is part of the definition of rational thought [@brashier2020; @koslowski1996]. Perhaps because it is so foundational to how we process the world, we also have trouble encompassing precisely what we mean when we say a relation is causal versus spurious.

<!--# The difficulties in defining causation are nothing new as causality has been a subject of intense philosophical debate for several centuries, if not millennia [@koslowski1996]. Rather, my point is that it is easy for scientists to ignore this source of uncertainty in discussions of causal inference, which leads to unrealistic standards for what a particular framework of causality can achieve. If instead we borrow from our own analysis of latent concepts, in which we can measure indicators but never the concept directly [@gerring2012, pp. 105-155], we can then conceptualize of causality as a latent scale with uncertainty over its "true" location. As such, it makes more sense for scholars to discuss research as being more or less causal, or communicating more or less causal knowledge, than granting some studies the "causally-identified" label while other studies are relegated to the merely exploratory.  -->

For this reason, I build an inductive definition of causal learning by looking at what I call three *silver* standards of causality: counterfactual/manipulationist, correlational, and mechanistic inference. While this material may be already well-known for some readers, due to the rapid development of causal inference methods and the dangers of using poorly-defined terms, I spend some time to be very clear about what I mean by "modes of inference." At the same time, due to space requirements and the inherent limitations of typology, I will no doubt leave out important edge cases or nuances in a vast corpus of thinking about inference.

# Counterfactuals and Manipulations

While its role in the social sciences was traditionally minor, manipulationalist and counterfactual causal inference has become the go-to reference for understanding causal relations. I consider these two paradigms, though conceptually distinct, to be grouped together as they are both fundamentally discussing the same thing. In brief, the counterfactual inference imagines an alternate world in which the causal factor is not present [@rubin1974; @holland1986], or what is known as the potential outcomes of a given unit, while the manipulationist account emphasizes human (or possibly divine) efforts to force causal factors to take on certain values [@woodward2003; @morgan2007]. These brilliant formulations are what launched the "credibility revolution" [@angrist2010; @imbens2015] and compels widespread support for RCTs across the social and biomedical sciences.

<!-- While counterfactual causal thinking has been around for some time, it has received its strongest expression in the potential outcomes literature associated with Rubin and Holland. We are told to imagine that a unit $i$ could exist simultaneously in one of two states: a treatment state $Y(1)$ in which $i$ receives a treatment and a control state $Y(0)$ where $i$ does not receive the treatment. Unfortunately, we cannot observe unit $i$ simultaneously in both states, both having and not having the treatment, which is known as the fundamental problem of causal inference. While this fundamental problem would seem to be just that, a fundamental problem, instead Rubin argued that randomly assigning units to receive the treatment $D=1$ or remain in control $D=0$ would provide an average estimate of this counterfactual difference:  -->

<!-- \hat{ATE} = E[Y(1|D=1) - Y(0|D=0)] -->

<!-- The reasoning is straightforward: if treatments are assigned randomly to subjects, then if we have a sufficiently large subject pool and take the average between the treatment and control groups, any one *individual* counterfactual is equally likely to occur in the treatment or control group.  -->

It is not necessary, of course, to use randomization within a counterfactual or manipulationist theory of inference: it is simply more difficult to know whether an intervention affected the outcome. In the potential outcomes framework, the concept of ignorability determines when an observational design without randomization is able to meet the standards of a randomized design by making missing potential outcomes ignorable [@przeworski2009; @imbens2015]. By doing so, the potential outcomes framework also influenced statistics in observational research, as I describe later.

@pearl2000 significantly expanded the definition and possibilities of manipulationist inference by his introduction of network relations in the form of directed a-cyclic graphs (DAGs) to stand for causal relationships. Pearl's main insight is that formulations of causal relations based on conditional probabilities alone cannot capture all of what is meant by causality because variables can be associated with each other without having any direct causal relationship. By providing a very rigorous definition to the notion that correlation and causation are separate phenomena, he came up with a framework that precisely relates manipulationist ideas of inference to statistical methods of estimating relationships between variables. However, Pearl's approach does not need to be restricted to manipulations or counterfactuals, as I show later. I group him in with this category as he shows a preference for manipulation and counterfactual inference as higher levels of knowledge compared to association [@pearl2018, see Figure 2 in Chapter 1].

To summarize Pearl's approach to establishing causality using manipulation, imagine that there are a set of variables $Z$, and a variable of interest $X$, all of which jointly cause an outcome $Y$. To come up with a directed acyclic graph (DAG), all of the variables that are causal factors must be pointing towards the outcome $Y$ or on some chain to $Y$, as in Figures 1 and 2.

```{r dag1,echo=FALSE,warning=FALSE,fig.cap='Confounded Example of a Directed Acyclic Graph',message=FALSE}
unctrl <- "X Y Z"
ctrl <- "X"

# paths
meas <- "X -> Y     
          Z -> X"
unmeas <- "Z -> Y "

output <- makeDAG(dagname='example1',filetype='png',
        text.nodes=unctrl,
        solid.edges = meas,
        dashed.edges=unmeas,
        embed=T,width = 1100, height=600)

knitr::include_graphics('example1.png',dpi=300)
```

Because all the variables are pointing in the same direction, these graphs are acyclic, or each graph can never return to its origin. A causal relation is defined explicitly as fixing one of the causal factors (the $do$ operator) so that only the manipulated variable affects the outcome, helpfully removing the confounding association between $X$ and $Z$ in Figure 1 as shown by the box around $X$ in Figure 2.

```{r dag2,echo=FALSE,warning=FALSE,fig.width=5,fig.cap='Identified Example of a Directed Acyclic Graph',message=FALSE}
unctrl <- "Z Y"
ctrl <- "X"

# paths
meas <- "X -> Y
          Z -> Y"
# call function silently
output <- makeDAG(dagname='example2',filetype='png',
        text.nodes=unctrl,
        box.nodes=ctrl,
        solid.edges = meas,
        embed=T,width=600,height=600)

knitr::include_graphics('example2.png',dpi=300)
```

This manipulation--forcing $X$ to a specific value--removes the influence of $Z$ and the "backdoor path" by which changes in $Z$ cause changes in both $X$ and $Y$. By visualizing these relationships, and providing an algorithm to convert the graph into observable probabilities, Pearl did a great service to causal inference practitioners. Yet while manipulation is certainly a core part of causality, it does not exhaust the subject.

Rather than cast aspersions on RCTs and other forms of counterfactual/manipulationist inference, it is best to think of these methods as providing a very helpful and important component of what is meant by causality. RCTs are one of the best means available for addressing unmeasurable selection attributes, such as situations where wealthier people tend to select into high-income neighborhoods, and vice versa. Under ideal conditions, such as with large samples, low attrition rates, and cleanly measured treatments, RCTs provide a very high amount of causal knowledge.

# The Relation Between Correlation and Causal Knowledge

Although at one time it was the dominant approach to causal inference in the sciences, the Humean conception of "constant conjunction" has fallen out of fashion. Hume supposed that we could not know why any two events occurred together and to infer any of this knowledge was a fallacy. Rather, all we could know was whether events tended to occur together. This correlational theory of inference was codified by Pearson [@pearl2018 , 53-91] and has remained a staple of statistical analysis: checking for associations between variables, or looking for risk factors, as in medicine [@boyko1990; @gershman2018]. While today's statistical education emphasizes that correlation does not equal causation, that tendency has not always been as strong among statistical practitioners.

The reason that correlation still matters even if we do not know the direction of causality or the mechanisms behind the correlation is because causation *does* imply association, of which correlation is a specific metric [@altman2015]. We may not be able to record the association between two variables due to confounders or selection (collider) bias, but if a causal relationship exists, then we can infer that at some level, somewhere, correlation must be happening: if $A$, then $B$. Conversely, if we can prove that two variables are perfectly uncorrelated, accounting for our knowledge of the causal process, then we have obtained good--though not perfect--evidence that a causal relationship probably does not exist.

<!--# For this reason, observational analysis does have a close relationship to causality even if it does not define it . When we observe a correlation among human-generated outcomes, we know that a causal process is likely at work, although we may not know which set of factors are the most important (i.e., causal identification). Statistical methods only give us a way to quantify the uncertainty in estimating the correlation, not in determining whether the association is "truly" causal. But if we abandon the notion that there is a gold standard for causality, then observational analysis, or searching for correlations, still matters for generating casual knowledge even if we cannot learn everything we want to learn.  -->

While some are willing to dismiss traditional statistical methods without randomization or at least manipulation of treatments, entire fields of science are based on purely observational analysis, including astronomy, forensic anthropology, paleontology, and so on. We have never manipulated giant bodies of gas to force them to explode, but we are still fairly sure we know what supernova are [@bethe1990]. The reason for this is that as a silver standard, observational analysis does provide causal knowledge if ideal conditions are met, as is true of randomized experiments. In particular, observational methods provide evidence of--though never fully determine--causal relations when we know which variables are relevant to the outcome and in what way, the data provide accurate measures of these variables, and we have as much data as we might want.

Researchers also often use syllogisms to interrogate models [@ashworth2021] and reason their way through situations where they cannot collect all the data they want, such as astronomers' disputes over the location and number of planets. If an orbit reflects certain instabilities, then a planet can be hypothesized to exist even if there is no direct evidence for it [@smith1989].

The re-orientation away from observation as a form of causal inference has led to the rise of the so-called quasi-experimental research designs. These methods, including difference-in-difference and regression discontinuity designs, are confusing to define because they make reference to experimental treatments but treatment assignment is never manipulated by the researcher. Instead, these models' superiority over other observational methods is that they can be expressed in Rubin's potential outcomes notation in a way that establishes ignorability [@lee2008; @abadie2005]. However, establishing the conditions of ignorability does not itself make those conditions more likely to be met, and practitioners seem to expect that these models will work "out of the box" better than other models.

Aside from the clear expression of the ignorability assumptions, such as parallel trends, these models are fundamentally derivations of well-known statistical models. Difference-in-differences is an interactive fixed effects model with panel data [@kropko2020]. Regression discontinuity design is a form of non-parametric regression where the predictor is evaluated at a single point [@lee2008; @calonico2014]. Causal identification in these models requires knowing a fair amount of detail about the true data-generating process (i.e., the causal graph), leading some to argue that these methods have become over-employed and poorly understood [@kahn-lang2018; @caughey2011; @grimmer2011]. To proxy for random assignment, scholars have come to emphasize statistical tests for incomparability between units, such as pre-treatment trends, but these statistical fixes can have the unfortunate side effect of encouraging type II errors given the low power of these tests [@roth2021; @angrist2019].

These models are certainly useful, but are best understood within the general framework of observational analyses that can provide causal knowledge given the right conditions rather than clever loopholes that allow a scholar to claim causality without an experiment. It is entirely possible to obtain causal knowledge from observational methods without having to resort to a quasi-experimental design. One oft-cited example of the ideal conditions for observational inference comes from the analysis of the correlation between smoking and lung cancer [@cornfield1959]. Another, although it is not often expressed this way, is the application of polling aggregation models to predict electoral outcomes [@campbell2016]. While election forecasting models are not perfect and sometimes over or under-predict, the underlying causal process is usually undisputed: if a person is asked who they will vote for the day before the election, they will very likely cast a vote for that person in the ballot box. In this situation, there is plenty of data, we can measure most of what we want to know, and the measures are fairly direct of the underlying outcome.

# Mechanistic Causation

While the previous two approaches are often contrasted as the only ways of thinking about causality in the sciences, there is a third philosophy that is gaining traction among qualitative researchers, especially in sociology and political science. There has been much work in these disciplines in recent years on establishing how case study research can identify mechanisms that link causal variables [@bennett2014; @abbott1992; @evera1997]. Again, instead of considering this line of inquiry to be a subsidiary issue to the question of causality, I propose that mechanisms are a part of what we mean by causality, and hence are their own silver standard. When ideal conditions are met, we can infer causality with reasonable, though never perfect, confidence.

Several definitions of mechanisms have been proposed in the literature [@mahoney2012; @gerring2017]. I use for this paper the standard of @waldner2015 that mechanisms are invariant processes connecting causal variables to each other. The example he uses is very instructive: the person who discovered the mechanisms through which aspirin relieved heart pressure, Sir John Vane, received a Nobel Prize in 1982, despite the fact that it was already known that aspirin had beneficial properties. This Nobel prize is puzzling under either the observational or experimental approaches to causality: if the strength of association was indisputable and random assignment had provided an estimate of all possible counterfactuals, then what questions remained for Sir John Vane to receive a Nobel for?

The answer is that when humans conceive of causality, we imagine there to be some kind of process linking cause and effect. Defining exactly what this is process is can be difficult, but the invariant standard is a helpful step. We are looking for processes that are so low-level that they operate similarly in all contexts. In a social scientific setting, we might think of basic emotions like fear, anger and happiness [@pearlman2013], or the rational actor model [@elster1994].

Waldner's theory of causation is particularly useful in this paper as it directly compares the mechanistic thinking of causation to Pearl's use of network diagrams for causal structures. To integrate mechanisms, we can introduce labels on the edges that identify which mechanism is in operation.[^1] Importantly, *these mechanisms are not variables*, as causal mediation analysis pre-supposes [@imai2010]. Rather, mechanisms are root processes that are at the limit of our observational capacity and are so fundamental as to be nearly determinate to the outcome. Causal mediation assumes we can collect ample data on the mediators, but if we are at the limit of our observational capacity, the scope of data collection is necessarily limited and the necessary quantification may result in important loss of information that is difficult to capture in a rectangular dataset [@collier2010]. It is of course always possible to improve the quantified measurement of mechanisms to the point that they can be treated as a mediators, which is a point of connection between mechanistic and other modes of inference.

[^1]: While @waldner2015 first proposed that edges could be labeled as representing mechanisms, Pearl has also come to the same interpretation. See @cinelli2019, footnote 2.

Like the other two silver standards, under ideal conditions mechanistic research can provide strong evidence of causality. In the social sciences this entails collecting exhaustive evidence on a person's decision-making in what has come to be called process-tracing. Process-tracing methodologists refer to the idea of a "smoking gun" as the kind of evidence that establishes causality within this framework, such as obtaining a private diary of an important leader that describes in detail why they made their decisions [@evera1997; @collier2010; @bennett2014; @humphreys2015]. These ideal conditions are relatively rare, but like experimental and observational approaches, we can have confidence in inferring a high level of causal knowledge when we have all the information we might want about how $X$ changed into $Y$.

<!--# For example, imagine if we wanted to know whether the U.S. bombing of Dresden influenced Hitler's strategies during World War II. In a qualitative framework, we could obtain a smoking gun if we had a private diary by Hitler's own hand that described verbatim his thoughts on the U.S. bombing of Dresden and whether or not he shifted war tactics in response to the bombing. If we then also had detailed evidence, such as transcripts of meetings and official orders, carrying out these changes in tactics prescribed in the journals, then we could state with a high degree of confidence that we know that the bombing of Dresden had a causal impact on Hitler's decision-making.  -->

Of course, qualitative inference does lend itself to single-act causation as opposed to establishing general trends between variables. However, to the extent that the variables under study are the same across conditions, then we could presume that our knowledge of the mechanisms linking these variables is similarly invariant. If we can establish what the mechanisms are then we can be more confident that the variables are causally associated in addition to any relationship we estimate using observational or experimental statistics.

# Synthesis

> "Of course it is happening inside your head, Harry, but why on earth should that mean that it is not real?"
>
> -- Professor Albus Dumbledore from J.K. Rowling's *Harry Potter and the Deathly Hallows*

The point of presenting each of these ways of inferring causality is to put forward a more liberal conceptualization of causal inference for the social sciences. To paraphrase the U.S. Supreme Court Justice Potter Stewart, we may not be able to define causality exhaustively, but we know it when we see it. In an exhaustive summary, @sloman2015 argue based on extensive experimental evidence that while the network perspective of Pearl solved many issues in formalizing causal thinking, it still "has not offered a silver bullet that answers all questions about human thought" (p. 240). What causality exactly means is a subject for debate, even if we have made remarkable strides in terms of precisely detailing certain causal logics in the past three decades.

In common parlance in the social sciences, a "causal inference" model implies either a randomized control trial or the usage of certain statistical methods that can be expressed using Rubin's potential outcomes notation. This slippage in terminology puts the cart before the horse: causality does not inherit from counterfactuals; rather, counterfactuals arose as a way of expressing what is meant by causality. No one framework has yet to define all of what we mean by causality, and if we do so, such as by claiming that RCTs are a "gold standard", we will inevitably end up deflating the value of other, silver, standards.

However, it may yet be possible to construct a more satisfying, over-arching framework. @spirling2022's advocacy for the "inference to the best explanation" framework is a compelling example of how we might be able to more directly integrate different research traditions into a single inference method. As such, I do not think it is necessary to adopt an approach like "causal pluralism" that does not want to define standards for causal inference [@reiss2009]. Rather, the aim is for a unitary conception of causation that can still allow for diversity in research methods [@gerring2006]. The aim of this paper is not to propose my own framework, but to allow the inference modes to exist even in their apparent contradictions, and use entropy to adjudicate the amount of causal learning each contains, as I demonstrate in the next section.

<!-- Nonetheless, causality will remain a latent concept: we cannot observe it, and our uncertainty over the term will never completely go away. Permitting this remaining uncertainty, and allowing the use of the word "causal" in more settings than just randomized control trials or potential outcome models, will help avoid mis-perceptions and researcher frustration. Ultimately, by replacing the descriptive/causal dichotomization with an emphasis on the relative amount of causal knowledge obtained from a study, we can incorporate this uncertainty while still upholding strict standards for inference. -->

# Entropy as a Neutral Standard

Even if we do not yet have an accepted way to integrate all of the silver inference modes into a signle framework, we can still compare research designs across modes. To accomplish this, I present the concept of entropy to formalize the idea that diverse research methods can provide varying amounts of causal knowledge, as opposed to delineating some methods as causal and others as observational. In general, entropy describes the decay of a system, such as gas molecules moving farther and farther apart to fill a sphere. Statistical, or Shannon, entropy applies the same concept to probability, providing a measure of the "information" in a random variable [@shannon1948]. In general, as probability distribution becomes more equal or uniform, entropy increases because all outcomes are equally likely, whereas when a probability distribution becomes more degenerate or peaked, entropy decreases as some outcomes are more certain than others.

Shannon entropy $H$ is defined as a simple formula applied to a distribution of $N$ probabilities that cumulatively sum to 1:

```{=tex}
\begin{equation}
\label{shannon}
H = - \sum_{n=1}^N p_n \text{log}p_n
\end{equation}
```
The formula in (\ref{shannon}) is unfortunately not intuitive, but its appeal is in meeting certain qualifications for determining an entropy measure of probability, including that it increases as it moves away from neutral probabilities over outcomes (such as $\frac{1}{N}$) and reaches a minimum at 0 when any of the probabilities are equal to 1. The units of entropy are determined by the type of logarithm employed. Because I am interested in entropy as a framework rather than with a particular empirical application, I use an unconventional logarithmic base of 1.01:

```{=tex}
\begin{equation}
\label{myv}
H = - \sum_{n=1}^N p_n \text{log}_{1.01}p_n
\end{equation}
```
A base of 1.01 means that every unit increase in entropy equals a one percent increase in entropy. Figure 3 plots entropy calculations for probability distributions with varying levels of total uncertainty or spreadout-ness. What is important to note is that all of these distributions have the same expected, or average, value, but are nonetheless very different statements about underlying uncertainty. Roughly speaking, the uniform distribution has 100 percent more entropy than the normal distribution, which has 30 percent more entropy than the student's T and Laplace distributions. These plots show why entropy is a powerful heuristic: it captures our sense of how certain we are of the empirical possibilities underlying a distribution of probability that is independent of the form of the distribution. If we know nothing about a process, we can assume a uniform distribution which leaves probability mass on all possible outcomes. But if we know more about how a process operates, we can considerably reduce our uncertainty (and hence entropy) by choosing a more specified distribution.

```{r dement, fig.cap="Entropy Calculations Based on Empirical Densities of Statistical Distributions"}
plot_d <- data_frame(Distribution=rep(c('Uniform',
                                        'Gaussian',
                                        "Student's T",
                                        "Laplace"),
                                      each=1000),
                     Variates=c(runif(1000,min=-10,max=10),
                                rnorm(1000,mean = 0,sd=2),
                                rt(1000,3),
                                rlaplace(1000)))
my_ent_func <- function(mydist) {
  counts <- discretize(mydist,100,r = c(-10,10))
  counts_p <- counts/sum(counts)
  # need to drop zeroes
  -sum(counts_p[counts_p>0]*log(counts_p[counts_p>0],1.01))
}
plot_text <- group_by(plot_d,Distribution) %>% 
  distinct %>% 
  mutate(Entropy=paste0('Entropy = ',round(my_ent_func(Variates),0))) %>% 
  select(Distribution, Entropy) %>% 
  distinct
# plot the density in a facet wrap
plot_d %>% 
  ggplot(aes(x=Variates)) +
  geom_density(alpha=0.5,fill='blue',colour=NA) +
  facet_wrap(~Distribution,ncol=2,
             scales='free_x') +
  geom_text(data=plot_text,aes(label=Entropy),x=0,
                            y=0.2,
            fontface='bold',
            size=3) +
  ylab('Density') +
  theme(panel.grid = element_blank(),
        panel.background = element_blank(),
        strip.background = element_blank(),
        strip.text = element_text(face='bold')) +
  labs(caption="Because these are continuous distributions and entropy is a measure\nof discrete random variables, the continuous variates were first binned and then converted to probabilities.")
```

While entropy has been applied successfully to many statistical problems, my intention in defining it here is to think of it as a way to understand the relative value of the causal paradigms previously discussed. Ultimately, the goal of the social sciences should be to reduce entropy whenever possible in terms of our understanding of how the social world operates. If we have more certain knowledge of the distribution of outcomes, we can state with reasonable confidence that our knowledge is increasing [@gerring2006]. To do so, we have to produce new propositions that explain human behavior and allow us to make judgments about what is more or less likely to occur.

The maximum entropy principle provides further clarity about how we can maximize knowledge while avoiding over-confidence. @jaynes2003 defines the maximum entropy principle as always preferring a distribution of higher entropy conditional on including all known facts in the distribution. For example, suppose we wanted to predict stock market prices. Lacking any special knowledge into stock prices, we would want our uncertainty to reflect the fact that all we have to analyze are the movements of individual stocks over time--we would want to maximize entropy, or uncertainty, given the data we have. But if we knew that the Federal Reserve intended to increase interest rates, we could include that information in our model and consequently obtain a lower entropy distribution.

In other words, we want to learn new facts about the world such that we reduce our entropy in understanding causal relations. At the same time, we want to maximize entropy given what we know to reduce blind spots and over-confidence. Causal inference involves striking this delicate balance between assuming too much and assuming too little.

This framework helps resolve some inconsistencies in how models are incorporated in the social sciences. On the one hand, more complex models are seen as embodying increased knowledge [@clarke2007; @slough2019]. On the other hand, there has been considerable push back at models that appear to be baroque and less easy to explain than tried-and-true ordinary least squares (OLS) regression [@angrist2008]. Maximum entropy helps explain these mixed feelings: we should prefer more complex models over simple models because our over-arching aim should be to reduce entropy, and more complex models have less entropy. On the other hand, we do not want to reduce entropy without a good reason lest we over-state our certainty [@frank2009].

To use entropy to understand causal paradigms, I return to Pearl's causal diagrams. My intention is not to suggest that Pearl's theory is the final take on causality, but rather that network relations are deeply intuitive representations of human thought patterns. As such, they are a helpful starting block for comparing very different representations of causality. There are existing applications of entropy to causal graphs, but the aim in the literature is to uncover hidden confounders given a set of observed data as opposed to making larger statements about research design [@kocaoglu2020; @tee2016; @wieczorek2019].

Let us imagine that we launched ourselves in a spaceship and landed in a foreign world. We have almost no prior knowledge of how people on this planet relate to each other, but we want to understand how the world's residents select their leaders. All we can do is come up with a list of plausible factors that might affect leader selection. Given our experience of such occurrences on earth, we come up with the following list of variables: political ideology (I), economic benefits (E), ethnic affinity (A), leader personal qualities (Q), and the risk of conflict (C). We can think of these variables as nodes in a network all connected with the outcome of leader selection (Y) as is shown in Figure 4.

```{r netgraph, fig.cap="Causal Diagram with Complete Uncertainty"}
mat <- matrix(rep(1,36),6,6)
row.names(mat) <- c('I',
                    'E',
                    'A',
                    'Q',
                    'C',
                    'Y')
colnames(mat) <- c('I',
                    'E',
                    'A',
                    'Q',
                    'C',
                    'Y')
diag(mat) <- 0
mat_edge <- matrix(rep('frac(1,36)',36),6,6)
netobj <- network(mat,directed=T)
set.edge.value(netobj,'Probability',mat_edge)

ggplot(netobj, aes(x = x, y = y, xend = xend, yend = yend)) +
  geom_edges(color = "grey50",
             arrow=arrow(length = unit(0.5, "lines"), type = "closed")) +
  geom_nodetext(aes(label=vertex.names),
                size=7) +
  geom_edgetext_repel(aes(label=Probability),parse = T,
                      label.size=NA) +
  theme_blank()

```

Each edge in this graph is labeled with the dual expected probabilities that a link exists in either direction, with each one labeled $\frac{1}{36}$ to represent our current ignorance, i.e., any link between any of the nodes is equally likely in either causal direction. The edges in this network are bi-directional to show that our in our state of complete uncertainty we cannot even say what the direction of causality could be. While the uncertainty in this figure is extreme, it comes closer to the actual state of social science research than the elegant causal graphs in many texts derived from mechanical examples.

Given the previous discussion, the question now is to reason about which method of causal inference to apply to Figure \@ref(fig:netgraph). The easiest way to answer this question, and one often chosen, is simply to choose whichever method best fits the researcher's skills and experiences. While very practical, it poses a chicken-and-egg problem, and only shifts the question to which paradigm researchers should invest in to gain experience and skills.

I propose that a better heuristic is to ask what would reduce entropy in the causal graph. There are very complicated entropy statistics for graphs, but to simplify matters I apply the entropy formula to each edge probability in Figure \@ref(fig:netgraph):

$$
-\sum_1^{36} \frac{1}{36} \text{log}_{1.01} \frac{1}{36} = 360
$$

We start with the considerably high number of 360. At this amount of entropy, we are not likely to be wrong, but we also cannot say much of value about our study of this foreign planet's society. Initially, let us consider a choice between an experimental and an observational analysis. Suppose than with an experiment we can determine directly the probability of the connection between ethnic affinity (A) and leader selection (Y).[^2] If we pull off a quality experiment, we can double the probability of the link from $A \rightarrow Y$ and reduce the probability of the link from $Y \rightarrow A$ (reverse causality) to 0. We can consider the experiment to be *causally identified* because it is possible, given enough experimental data, to either rule out or establish the relationship between these two nodes [@keele2015]. Then we can re-calculate our entropy measure, which shows a decrease in entropy of 4 percent:

[^2]: We ignore for the time being the difficulty in enacting these research designs.

$$
-[(\sum_{1}^{34} \frac{1}{36} \text{log}_{1.01} \frac{1}{36}) + \frac{2}{36}\text{log}_{1.01}\frac{2}{36}] = 356
$$

However, suppose that if we conducted an observational data analysis, we could increase or decrease the probability of the links between leader selection and economic benefits (E), leadership quality (Q), ideology (I), ethnic affinity (A) and conflict (C) from $\frac{1}{36}$ to $\frac{1.5}{36}$ while lowering the opposite links to $\frac{0.5}{36}$. This research design is not causally identified because we cannot be sure, i.e. we cannot know with probabilities approaching one, what effect any one of these variables have on the outcome, *only their joint distribution*. This analysis would result in the following change in entropy:

$$
-[\sum_{1}^{26} \frac{1}{36} \text{log}_{1.01} \frac{1}{36} + \sum_{1}^{5} \frac{1.5}{36}\text{log}_{1.01}\frac{1.5}{36} + \sum_{1}^{5} \frac{0.5}{36}\text{log}_{1.01}\frac{0.5}{36}] = 356
$$

In other words, in this example, the observational and experimental studies would have similar effects on reducing the entropy of the total system *even though they made very different statements about the underlying causal structure*. Intuitively, we can learn a lot from establishing a specific causal link in a specific direction with a high degree of certainty, but we can also learn a lot from examining associations between variables, even if we cannot arrive at conclusive predictions. The point of this exercise is not to suggest that observational methods are better than experimental methods, but rather that the value of each depends on the nature of the causal problem, and it is *not* always the case that experiments produce more causal knowledge than observational studies. From a Bayesian point of view, we could re-state this problem as meaning that we should always prefer the research design that increases our knowledge relative to our prior, even if the knowledge we obtain has residual bias [@little2018].

<!-- We can further extend this discussion by considering the mechanisms implied by the edges. Instead of treating the edges as solely representing causal relationships, we could imagine a distribution over mechanisms for each edge. We could similarly decrease our entropy over mechanisms by learning which mechanisms are the most likely and least likely for a given node. Uncertainty over mechanisms could then be weighted with the overall association probability as entropy is additive on the log scale. -->

I now formally define the causal entropy measure as the Shannon entropy $H(\cdot)$ of an $N$ ordered set of variables $\{x_1, ... x_N\}$ that can be represented by a causal graph $V$ which meets all of Pearl's requirements: it includes a set of directed edges $e \in E$ between variables in the graph, and is acyclic. We can then take the Shannon entropy $H(\cdot)$ of the joint distribution of these variables, which can be denoted $P(v)$, following Pearl's notation:

```{=tex}
\begin{equation}
\label{entgraph}
H(P(v)) = - \sum_{i=1}^N P(x_i | pa_i) P(pa_i) \text{log}P(x_i | pa_i) P(pa_i)
\end{equation}
```
Where the notation $P(x_i | pa_i)$ indicates that each component of $P(v)$ is the conditional distribution of each variable $x_i$ in $V$ with respect to the set of its ancestors $pa_i$ that are causally relevant to $x_i$. This formalism captures the procedure done earlier in which probabilities are assigned to each directed relation in a causal graph. Because $P(v)$ is a joint distribution over all such relations, it meets the requirement that the probabilities of all of the causal relations sum to 1.

Formally, we can then consider research designs as representing different joint distributions, such as $P(v)$ and $P'(v)$. We can state that a research design that results in $P(v)$ creates less causal knowledge than a research design that produces $P'(v)$ iff:

```{=tex}
\begin{equation}
\label{greaterent}
H(P(v)) > H(P'(v))
\end{equation}
```
The one complication is that entropy is only defined over discrete variables. However, it is straightforward to calculate entropy of a continuous variable through a binning procedure, and the measure is available via a wide array of statistical software packages [@hausser2021].

As mentioned previously, we can consider a distribution of mechanisms in the causal graph $V$, which I denote as the set $\Omega$. Each directed relation $P(x_i | pa_i)$ would have a corresponding distribution over mechanisms $\Omega$, where $P(\Omega|x_i,pa_i)$ represents the joint distribution of all mechanisms for a given relation. Because of causal graphs' Markovian property [@pearl2000, Ch. 1], we can simplify the expression to $P(\Omega|x_i)P(pa_i)$ because each variable in a causal graph is independent of other variables in the causal graph once we factor in its parents. In other words, we only need to consider the mechanisms of each node but not each preceding node. We can then consider the joint of both distributions:

```{=tex}
\begin{equation}
\label{entmech}
H(P(v, \Omega)) = -\sum_{i=1}^N P(x_i | pa_i) P(\Omega|x_i)P(pa_i) \text{log}P(x_i | pa_i)P(\Omega|x_i)P(pa_i)
\end{equation}
```
If we consider a graph $V$ that had identical distributions for the probabilities of causal relations $P(v)$, but different distributions for mechanisms $P(\Omega')$, we could then define when a study that increased our understanding of mechanisms would be preferable:

```{=tex}
\begin{equation}
\label{greatermech}
H(P(v, \Omega)) > H(P(v, \Omega'))
\end{equation}
```
So far I have shown all results using Shannon entropy $H$. It is also possible to examine the Kullback-Leibler (KL) divergence [@kullback1951] between different probability distributions of $P(v)$. This measure of *relative* entropy is quite similar to Shannon entropy, so I do not address this further. Using the KL divergence could be useful in a situation where comparisons are made between multiple competing research designs rather than just two or three, and the aim is to compare distances across probability distributions.

The intention of this formulation is to show that the concept of statistical entropy sheds light on the difficult decisions that must be made when considering research designs. Although there may be times when directly calculating changes in entropy from causal graphs is necessary, the formalism can also be used as a heuristic for evaluating research designs without having joint probability distributions. Ultimately, the criterion of reducing entropy suggests that we aim for maximizing the amount we can learn, i.e. causal knowledge, from an application of any of the paradigms. Even weakly causally-identified designs can contribute causal knowledge. To illustrate the principle further, I apply the theory to two important recent areas of research.

## Case Study: COVID-19 Vaccines

The outbreak of the COVID-19 pandemic offers an important test case for understanding how researchers employed research designs to understand and prevent COVID-19 infections. Because of the speed of the outbreak and the enormous scale of research efforts, there was relatively little time for traditional disciplinary norms to determine research designs. The SARS-CoV-2 virus did not care for disciplinary preferences, forcing researchers to employ whatever methods they had available to study the pandemic. The extreme pressures of this exogenous shock is the reason why I choose this particular area of research even though it is not a part of the social sciences proper. Under pressure, scientists went with their causal intuitions as opposed to relying on traditional disciplinary hierarchies, producing innovative work that built across modes of inference rather than relying on one mode at the expense of others. In addition, the massive levels of funding available from governments overcame one of the most common non-methodological factors in determining research designs, permitting issues of inference to become relatively more important.

Although there are many possible research questions, in this case study I focus on one crucial area: the development of vaccines. At first blush, it would seem that vaccines are a relatively straightforward exercise in terms of research design. After months of development, the drug companies Pfizer-BioNTech and Moderna released studies describing massive RCTs employing hundreds of thousands of volunteers over months. These studies provided concise and clear numbers concerning *vaccine efficacy*, or the ratio of infected individuals in the treatment group to the number infected in the control group [@polack2020; @baden2021]. Because the control group never received a vaccine, the difference between the two groups could be directly attributed to the drug.

In causal terms, this RCT solved a difficult yet well-known problem in studying COVID-19 infections: those who voluntarily participate in a COVID-19 vaccination study could be either more or less likely to be infected compared to those who would not want to volunteer for a vaccine. To give just one example, younger individuals showed less interest in vaccines compared to older individuals, and were also much less likely to become severely ill. On the other hand, younger people may have been more likely than older people to contract a COVID-19 infection because they had less fear of serious illness or death. As a result, any naive comparison of a group of volunteers and the general population could end up conflating age differences with vaccine uptake [@hodgson2021; @baack2021]. This causal identification problem is a straightforward example of confounding, as shown in Figure \@ref(fig:dag1). Any variable which could explain both vaccine uptake and the incidence of COVID-19 would be a confounding variable, and without confidence that we can collect data on and measure all confounding variables, we may not be able to identify the direct relationship between the vaccination and efficacy.

As is well-known, the Pfizer-BioNTech and Moderna trials proved to be a paragon of RCT methods, showing remarkably strong effects of the vaccine on efficacy, above 90%. At this point in the narrative, it would seem that RCTs had proven themselves as the gold standard: we had established that the vaccines worked, and now we could move forward with ending the pandemic. Indeed, such sentiments were common when the vaccines were introduced, leading to a relaxation of restrictions in the summer of 2021 [@bauer2021; @tregoning2021].

Fairly quickly, however, it became evident that the RCTs themselves were not sufficient to answer all the questions about vaccine efficacy. There were two main problems: first, people wanted to know how the vaccine performed *in the population*, which required an attention to the confounding variables that the RCT successfully ignored [@hungerford2021]. Second, the arrival of vaccine variants forced a re-evaluation of the vaccines' efficacy as RCT trials could not be run fast enough to keep up with new variants [@andrews2022]. These issues required both observational and mechanism-based modes of inference, as I will explicate below.

|                | I = Infected | I = Not Infected |
|----------------|--------------|------------------|
| T = Vaccine    | .15          | .85              |
| T = No Vaccine | .85          | .15              |

: $Pr(I|T,Z=\text{Old})$

We can examine the entropy reductions of these different interventions on the causal graph by detailing the causal variables' conditional probability distributions. For simplicity, I will take as my starting point the causal graph in Figure \@ref(fig:dag1), where the nodes can be relabeled so that there is one treatment variable, $V$ for vaccine, one outcome, $I$ for infection, and a confounder $Z$ , which I will consider to be age. To simplify matters, I will treat each variable as having two discrete values. Tables 1, 2 and 3 show plausible marginal conditional probability distributions for the three variables. To analyze the relationships in the causal graph, we need to consider two conditional probabilities, $Pr(I|V,Z)$ and $Pr(T|Z)$. Because the first conditional probability involves two conditioning variables $V$ and $Z$, I separate this distribution of $I$ into two separate tables for Young and Old subjects as can be seen in Tables 1 and 2.

|                | I = Infected | I = Not Infected |
|----------------|--------------|------------------|
| T = Vaccine    | .02          | .98              |
| T = No Vaccine | .98          | .02              |

: $Pr(I|T,Z=\text{Young})$

|           | T = Vaccine | T = No Vaccine |
|-----------|-------------|----------------|
| Z = Young | .1          | .9             |
| Z = Old   | .9          | .1             |

: $Pr(T|Z)$

I assume here that these are the true probabilities of treatment efficacy and the confounding effect of age on vaccine uptake. It is important to note, too, that age also affects vaccine efficacy, with the vaccine more efficacious among the young than the old, as studies have shown [@bell2022]. To calculate entropy, we will need to start with a prior distribution representing what we think these relationships could be, or what I will call the null graph. For simplicity, I will assume a uniform prior for the null graph, i.e., that all of the probabilities in the tables are equal to exactly 0.5. While not shown, I can calculate the entropy by considering the full joint distribution of the causal graph, which involves creating a much larger table for all values of $Z$, $T$ and $I$, i.e. $P(I,Z,T) = P(I|T,Z) P(T|Z)P(Z)$. For reference, I also assume that the population is 25% young and 75% old.

```{r calc_ent_caus1}

prob_out_true <- tibble(pr_joint=c(.25*.02*.1,
                              .25*.98*.9,
                              .25*.98*.1,
                              .25*.02*.9,
                              .75*.15*.9,
                              .75*.85*.1,
                              .75*.85*.9,
                              .75*.15*.1),
                   I = c("I","I","H","H","I","I","H","H"),
                   `T` = c("T","C","T","C","T","C","T","C"),
                   Z = c("Young","Young","Young","Young","Old","Old","Old","Old"))

prob_out_prior <- tibble(pr_joint=rep(1/8,8),
                   I = c("I","I","H","H","I","I","H","H"),
                   `T` = c("T","C","T","C","T","C","T","C"),
                   Z = c("Young","Young","Young","Young","Old","Old","Old","Old"))
                   
prob_out_treat <- tibble(pr_joint=c(.5*.02*.1,
                              .5*.98*.9,
                              .5*.98*.1,
                              .5*.02*.9,
                              .5*.15*.9,
                              .5*.85*.1,
                              .5*.85*.9,
                              .5*.15*.1),
                   I = c("I","I","H","H","I","I","H","H"),
                   `T` = c("T","C","T","C","T","C","T","C"),
                   Z = c("Young","Young","Young","Young","Old","Old","Old","Old"))

ent_orig <- -sum(prob_out_prior$pr_joint*log(prob_out_prior$pr_joint,1.01))
ent_true <- -sum(prob_out_true$pr_joint*log(prob_out_true$pr_joint,1.01))
ent_treat <- -sum(prob_out_treat$pr_joint*log(prob_out_true$pr_joint,1.01))

prob_out_true_mech <- bind_rows(prob_out_true, prob_out_true, .id="Omega") %>% 
  mutate(pr_joint=pr_joint*rep(c(0.1,0.9),each=n()/2))

prob_out_mech_prior <- bind_rows(prob_out_prior, prob_out_prior, .id="Omega") %>% 
  mutate(pr_joint=pr_joint*rep(c(0.5,0.5),each=n()/2))

prob_out_true_no_mech <- bind_rows(prob_out_true, prob_out_true, .id="Omega") %>% 
  mutate(pr_joint=pr_joint*rep(c(0.5,0.5),each=n()/2))

prob_out_treat_no_mech <- bind_rows(prob_out_treat, prob_out_treat, .id="Omega") %>% 
  mutate(pr_joint=pr_joint*rep(c(0.5,0.5),each=n()/2))

prob_out_treat_mech <- bind_rows(prob_out_treat, prob_out_treat, .id="Omega") %>% 
  mutate(pr_joint=pr_joint*rep(c(0.1,0.9),each=n()/2))

ent_true_mech <- -sum(prob_out_true_mech$pr_joint*log(prob_out_true_mech$pr_joint,1.01))
ent_prior_mech <- -sum(prob_out_mech_prior$pr_joint*log(prob_out_mech_prior$pr_joint,1.01))
ent_true_no_mech <- -sum(prob_out_true_no_mech$pr_joint*log(prob_out_true_no_mech$pr_joint,1.01))
ent_treat_no_mech <- -sum(prob_out_treat_no_mech$pr_joint*log(prob_out_treat_no_mech$pr_joint,1.01))

ent_treat_mech <- -sum(prob_out_treat_mech$pr_joint*log(prob_out_treat_mech$pr_joint,1.01))
```

Calculation of Shannon entropy (not shown) indicates that the null prior graph has an entropy of `r round(ent_orig, 0)`, while the true graph has an entropy of `r round(ent_true, 0)`. These two figures give us the relative space within which we can plausibly learn about this outcome. If we end up with an entropy of less than `r round(ent_true, 0)`, we will be over-confident, inferring causality to what are in fact random events. Respecting the lower bound reflects the principle of maximum entropy discussed earlier: we should not want to be more certain of conclusions than the underlying causal process permits.

We can directly calculate the reduction in entropy for the experimental analysis of the vaccine by inserting values of the conditional probability distributions from the true graph for $P(I|T,Z)$ into the null graph. By adding in the true values for the conditional distribution, not just the average treatment effect, I assume that the treatment was high-powered enough to inform us about the true joint distribution of both the treatment and the potential confounder, age, as seemed to be true for most of the vaccine trials with tens of thousands of subjects enrolled. The entropy of this high-powered experimental analysis is `r round(ent_treat,0)`, which is only 19% larger than the true entropy. As such, the experimental technique was clearly a powerful way of learning about this causal process.

```{=html}
<!-- First, consider the outcome of the massive number of experiments performed on elections, canvassing and voter behavior in American politics. @kalla2018 recently documented how 49 field experiments produced an average null effect of campaign advertisements and mailings on voter behavior. This finding is of course puzzling as campaigns spend a large amount of money to do what this study says is on average unlikely to occur: affect voter choices in an election.

This study is fascinating as it represents a field where the application of experiments has reached a far greater depth than other areas of political science. In this case, it would seem that we could reduce entropy to a greater degree in the study of voter behavior by looking at other methods than experiments, either observational large-N analysis or qualitative research aimed at identifying subsets of voters for interviews. Part of the problem, as the authors of the study note, is that there is widespread treatment heterogeneity, which is shorthand for a large number of background factors that also affect the success of the treatment. Observational and qualitative analysis could help uncover what these background factors are, such as broad geographical, economic or cultural factors, or very specific group or individual-level mechanisms that the treatment is interacting with. In other words, observational analysis could help situate the experimental findings within a broader theory as some have called for in psychology [@muthu2019].

Conversely, we can consider a situation where observational and qualitative analysis has been remarkably widespread: the study of democratization. @coppedge2012 documents the wide variety of statistical models fit to ever-increasing datasets, including most recently the Varieties of Democracy of Project [@vdem2017]. In addition, countless country-level case studies of democratic processes exist in the scholarly literature dating back to the origins of comparative political science [@Moore2003]. Yet there are relatively few if any experiments on building democratic institutions and on encouraging support for democracy as opposed to authoritarian politics, which suggests that entropy could be further reduced in this field by considering more experimental approaches. -->
```
However, the analysis left important information unanswered on the causal graph, in particular what the relationship is between vaccine uptake and age. While the relative entropy would seem small, it is still an appreciable amount, and when vaccines were deployed to the population, it became a crucial factor for understanding the success of the vaccines [@hodgson2021]. If sicker people were more likely to take the vaccine, then the measures of vaccine efficacy from the total population would understate the efficacy of the vaccine. Understanding how the vaccine interacted with population demographics required obtaining data about vaccine uptake in the "real world" [@chodick2021], especially to combat misinformation about the efficacy of the vaccine by anti-vaccination groups. For this reason, it is clear that even though the most important question about the vaccine was answered by an RCT, there was ample room for observational studies that collected data on the spread of the vaccine and relative rates of COVID-19 incidence in the population.

Ultimately, these observational studies were necessary to uncover the remaining entropy in the causal graph, equivalent to a `r round(ent_treat,0) - round(ent_true,0)`% reduction in entropy. While this reduction was not in large as the reduction due to the experiment, it is important to note that this reduction could not be obtained from the experiment itself as it involved fixing the vaccine node $V$ to a particular value, such as with Pearl's $do$ operator. As argued previously, causal identification is sufficient for causal knowledge to be obtained, but it is not necessary. In this case, causal identification by fixing $V$ to a specific value in an RCT prevented any analysis of the $P(T|Z)$ relationship because by definition it removed that causal arrow from the graph. For this reason, an observational analysis that established the $P(T|Z)$ relationship---varying vaccine interest by age---would likely be labeled as descriptive, not causal. However, this distinction is relatively arbitrary when we consider the causal graph holistically using a measure like entropy.

Finally, it is important to note that mechanistic analysis also played an important role in determining the efficacy of vaccines in the pandemic. As mentioned earlier, the success of the vaccines waned depending on the mutations of the virus, and it was infeasible to keep running large RCTs for each variant. This is a kind of threat to inference that is rarely discussed, and could be described as "temporal validity" [@munger2019]. To address this problem, scholars examined whether the same mechanism underlying the vaccine's efficacy also occurred in the same way with variants, namely, the production of virus-neutralizing antibodies. These studies were not necessarily statistical in nature, involving close examination of relatively small numbers of petri dishes with new SARS-CoV-2 variants in blood that had vaccine-induced antibodies [@yadav2021; @hoffmann2022]. Furthermore, these studies could not be directly integrated into the causal graph examined above because they refer to factors that are not present on the graph itself, i.e., minuscule changes in antibody levels.

We can expand the analysis to incorporate the antibody mechanism if we give it two values, High and Low. With these two values in the set $\Omega$, we can then calculate the entropy of the combined causal graph conditional on this mechanism for the relationship between vaccine $V$ and infections $I$:

```{=tex}
\begin{equation}
H(P(I,Z,T,\Omega)) = H(P(I|T,Z)P(\Omega|T)P(T|Z)Pr(Z))
\end{equation}
```
If we start with a null graph where the probability of $\Omega=\text{High}$ is 0.50, and the true value is 0.9, then we have null and true entropy values of `r round(ent_prior_mech,0)` and `r round(ent_true_mech,0)` respectively. If we performed the experiment successfully but without learning about mechanisms, we would obtain an entropy value of `r round(ent_treat_no_mech,0)` while an experiment that also involved learning about mechanisms would result in an entropy of `r round(ent_treat_mech,0)`, or `r round(ent_treat_no_mech,0) - round(ent_treat_mech,0)`% less. As can be seen, even with a single mechanism, relatively large reductions in entropy are possible by obtaining evidence of its true value. The reason for this large reduction is due to the fact that we know that the mechanism must be present for the causal story to hold about immunological response. If we were less certain about whether these mechanisms mattered in this case, we would not see as large reductions.

Of course, it could always be possible to look at antibodies as a mediator and expand the causal graph, allowing us to use mediation analysis to formally test for whether antibodies mediate the vaccine [@imai2010; @tj2016]. What is important, however, is that statistical tests were not necessary to make evidentiary statements about the presence or absence of the proposed mechanism. The proximity of observation and the logical necessity of antibody levels changing helped obtain causal inference about the performance of the vaccine against new variants even if there was no uncertainty interval or p-value attached. Statistical methods may not work well in this mode of inference because the attention to micro-processes often entails serious limitations in data collection in favor of richly textured information [@collier2010]. The use of priors can permit a form of Bayesian inference, though it is technically more of a logical approach based on probability theory than a statistical method per se [@humphreys2015]. In any case, assigning a probability value to the mechanism $\Omega$ ultimately involves the researchers' qualitative judgment, not data collection. By maximizing knowledge of mechanisms, researchers were able to warn about dangerous variants long before either observational or experimental studies could be completed.

The point of this case study was not to argue that one mode of inference was superior to the other, but rather how each kind--experimental, observational and mechanistic--played a valuable role in causal learning about the efficacy of the vaccine. At different stages in the pandemic, each of these modes of inference helped determine the relative vaccine efficacy both against variants and in the population as a whole. While the RCT achieved the highest reduction in entropy, other modes of inference had an important role to play. Once cost-benefit factors are included, such as the need to learn about efficacy against new variants for the purposes of booster shots, smaller overall changes in entropy can still be very important from a social welfare perspective.

## Case Study: Oil Wealth and Democratization

One of the most actively-research questions in comparative political science concerns the theorized relationship between a country discovering oil resources and subsequent resistance to democratization. Known as the rentier state theory [@mahdavyhossein1970; @therent1987], this influential hypothesis has received both significant support [@ross2001; @ross2012] and resistance from scholars [@haber2011; @cherif2015]. Of particular relevance for the application of causal entropy is the recent work of @waldner2021, who review the literature and produce a compelling causal graph which is able to summarize much of the debate while also making their own claims about the underlying causal structure.

I reproduce their article's Figure 5 in Figure \@ref(fig:resourcedag) where variable $A$ stands for "autocratic resilience", $B$ for "British policy", $P$ for "protection", $O$ for "oil" and $S$ for "survival." Waldner and Smith's argument is that there is no actual causal arrow between oil and autocratic resilience. Rather, there appears to be such a relationship because of what is known as collider bias (or selection bias) caused by variable $P$. Waldner and Smith make the case that British policy caused autocratic resilience among the heavily authoritarian Gulf states, and the oil is associated with the authoritarian institutions because oil endowments helped these states secure protection from foreign rivals such as Saudi Arabia and Iran. As a result, these conservative monarchies were more likely to survive and more likely to be authoritarian, inducing a non-causal association between oil resources and authoritarian institutions.

```{r resourcedag,fig.cap="Replication of Figure 5 from Waldner and Smith (2020)"}

resource_dag <- "dag {
                      O -> P
                      P -> S
                      B -> P
                      B -> A
                      }"

rd <- dagitty(resource_dag)
ggdag(rd) + theme_tufte() +
  labs(x="",y="") +
  theme(text=element_text(family=""),
        axis.text = element_blank())
```

The aim of this case study is not to adjudicate this original claim, but rather to use their causal graph as a basis for evaluating possible research studies using causal entropy. This causal graph is far more involved than the previous one, and as a result the conditional probability tables are more complex. To simplify matters, we will focus on the probability of authoritarian resilience ($A$) conditional on British policy ($B$) and survival $S \in \{\text{High},\text{Low}\}$:

|                              | $A = \text{Endure}$ | $A = \text{Democratize}$ |
|-----------------------------|-------------------|------------------------|
| $B = \text{Intervention}$    | 0.5                 | 0.5                      |
| $B = \text{No Intervention}$ | 0.5                 | 0.5                      |

: $Pr(A | B , S = \text{Low})$

|                              | $A = \text{Endure}$ | $A = \text{Democratize}$ |
|-----------------------------|-------------------|------------------------|
| $B = \text{Intervention}$    | 0.5                 | 0.5                      |
| $B = \text{No Intervention}$ | 0.5                 | 0.5                      |

: $Pr(A | B , S = \text{High})$

As can be seen in both tables, given that this is a new argument, we will use as our prior distribution complete uncertainty. Otherwise we would be biasing our results in favor of Waldner and Smith's contention. Furthermore, there is an additional problem with arriving at a more informative prior graph: we cannot manipulate these variables, and we also cannot observe variation naturally. The Gulf states have never not been sovereign since their independence, and British policy in question occurred between the 1920s and 1950s as the monarchies consolidated their rule. Because we cannot go back in time to collect more data, we are much more limited in what tools we can use to arrive at more informative distributions for our causal graph.

```{r calc_ent_authorit}

prob_out_post <- crossing(A=c("Endure","Democratize"),
                          B=c("Intervention","No Intervention"),
                          S=c("Low","High"),
                          Omega=c("Interventionist","Non-Interventionist")) %>% 
  mutate(pr_joint=case_when(Omega=="Interventionist" ~ .5^3 * .9,
                            TRUE ~ .5^3 * .1))

prob_out_null <- crossing(A=c("Endure","Democratize"),
                          B=c("Intervention","No Intervention"),
                          S=c("Low","High"),
                          Omega=c("Interventionist","Non-Interventionist")) %>% 
  mutate(pr_joint=.5^4)

ent_orig <- -sum(prob_out_null$pr_joint*log(prob_out_null$pr_joint,1.01))
ent_post <- -sum(prob_out_post$pr_joint*log(prob_out_post$pr_joint,1.01))
```

As a result, the mechanisms are very important. Waldner and Smith present evidence on a crucial mechanism--the nature of British intervention in Gulf politics. In other words, what were the intentions of British policymakers towards these states? Did they have an explicit policy of promoting authoritarianism? We can label this mechanism $\Omega = \{\text{Interventionist},\text{Non-Interventionist}\}$, and then consider the joint distribution $Pr(A,S | \Omega) Pr(\Omega | B) Pr(B)$. While evaluating the quality of Waldner and Smith's historical work is beyond the scope of this short study, we will give them the benefit of the doubt and say that they can establish the $Pr(\Omega | B = \text{Interventionst} = 0.9)$. We can then calculate the reduction of log entropy for $Pr(A,S | \Omega) Pr(\Omega | B) Pr(B)$ vis-a-vis a null mechanism of $Pr(\Omega = \text{Interventionst} | B) = 0.5)$, which leads to a reduction in entropy from `r round(ent_orig,2)` to `r round(ent_post,2)`. As a result, their process-tracing reduces our uncertainty considerably, although learning about only one mechanism still leaves substantial uncertainty.

Another way we can learn about this causal graph is to focus on the hypothesized null relationships. The graph in Figure \@ref(fig:resourcedag) has no direct connection between oil $O$ and authoritarian resilience $A$. There have been experimental and quasi-experimental studies that have studied a hypothesized mediator known as demands for accountability in which higher reliance of the state on rents will cause reduced demands for accountability. We can then update our causal graph with this hypothesized relation in Figure \@ref(fig:resourcedag2) by denoting demand for accountability as $D$.

```{r resourcedag2,fig.cap="Causal Diagram with Hypothesized Mediator $D$"}

resource_dag2 <- "dag {
                      O -> P
                      P -> S
                      B -> P
                      B -> A
                      O -> D
                      D -> A}"

rd2 <- dagitty(resource_dag2)
ggdag(rd2) + theme_tufte() +
  labs(x="",y="") +
  theme(text=element_text(family=""),
        axis.text = element_blank())
```

We now have an open causal pathway between oil resources $O$ and authoritarian resilience $A$. However, because we still have the collider relationship with protection $P$ and survival $S$, we should be worried about studies that do not take into account this possibility for selection bias. Experimental inference would seem to be a good option here, although we are limited in what we can focus on in the graph. We could either try to manipulate oil resources (or find plausibly exogenous increases in natural resources) and see whether demands for accountability change (path $O \rightarrow A$), or we could look at whether increases in demands for accountability leads to lower authoritarian resilience (path $D \rightarrow A$).

Neither is particularly easy to achieve, though recent work has shown it can be done. @paler2013 employed an experimental treatment to prime Indonesians into thinking either about taxes or "windfall" natural resources as a proportion of their local government's budget. She found a 5-6 pp increase in accountability actions by survey respondents in the taxation versus windfall condition. On the other hand, @delacuesta2019 deployed a lab-in-the-field experiment with a similar informational treatment in Ghana and Uganda, but found little difference between taxation and other sources of revenue when it came to demands for accountability.

Estimating the $D \rightarrow A$ path may be even harder as it requires measures of authoritarian persistence. @kubinec deployed an interactive informational treatment during Algeria's national protest movement in 2019, and found that informing Algerians about the extent of government redistribution had strongly varying effects by respondent wealth. More wealthy respondents were less willing to join protests opposing the regime, while poorer respondents became more likely to do so following the treatment. In sum, the experimental evidence appears to be mixed on this question, possibly justifying Waldner and Smith's lack of a causal arrow from $O \rightarrow A$.

Finally, we could also consider studies that are observational in nature, such as @ross2001 and @haber2011. From the causal graph, it should be clear that these studies will be unable to be causally-identified because it is not possible to adjust for selection bias with "control" variables in regression [@hünermund], and a selection model would require at least some observations of countries that did not survive [@waldner2021, p. 898]. As such, we would have to interpret observational studies that measure the association between natural resources and authoritarianism as assessing the *joint* distribution $Pr(A,P,S,B|O)Pr(O)$--in other words, we can only factor out oil resources. One advantage is that oil resources have considerable variation, but a disadvantage is that oil resources change little over time. As such, our learning from these studies predominantly depends on how much oil varies vis-a-vis the joint distribution, and in any case, we will not be able to isolate the effect of oil on authoritarianism apart from the proposed selection process.

As such, in this case study it would appear that experimental approaches and qualitative or mechanistic studies would be most aptly suited to research progress--unless observational work was able to better model the possible selection processes. Simply observing oil resources is unlikely to offer much causal learning without an ability to adjust for other important variables.

# Conclusion

Ongoing debates about causal inference threaten to create divides that impede research progress. Part of the problem is the growing assumption that causal inference requires an RCT or at minimum a model expressed in terms of potential outcomes. This divide separates research into causal and "mere" association, with the former preferred over the latter without reference to the relative amount of causal knowledge to be gained.

Rather than point to problems with RCTs as a reason to distrust them, I aver that the underlying issue is that we conceive of causality as a binary process: either a piece of research is or is not causal. It is not that RCTs have more issues than are commonly acknowledged, but rather that the definition of RCT as the gold standard means that we artificially deflate the value of other modes of causal analysis. We should take a charitable approach by admitting that various theories and practices of inference contain varying amounts of causal knowledge, with the amount varying in relation to the credibility of the research design. However, as the maximum entropy principle shows, we cannot evaluate the relative causal knowledge obtained without a consideration of what is and is not known about a given causal system.

The principle of entropy provides one helpful framework by imagining the benefit of a study from the relative reduction in entropy it achieves. By using such a continuous criterion, we can admit that we can learn from studies that are not causally identified without lowering standards of inference. This framework can help guide both qualitative and quantitative decisions about formulating research inquiries, depending on the level of formalization needed.

\newpage

# Appendix

In this section I show R code that can produce causal graphs, estimate conditional probability tables, and calculate entropy quickly and easily. To do so, I use four R packages: `dplyr` for data management, `HydeNet` for creating conditional probability tables, `dagitty` for defining causal graphs and `ggdag` for creating visualizations of causal graphs. Each of these packages has many more options and possibilities than I show in this brief tutorial.

```{r setup2, echo=TRUE}
library(dplyr)
library(HydeNet)
library(dagitty)
library(ggdag)
```

I further define a convenience function for calculating entropy given a list of vectors or tables of conditional probabilities. The function's arguments include a list of each conditional probability table, the list of values for each variable  and the ability to set the base of the logarithm used, which defaults to 1.01.

```{r entropy, echo=TRUE}
entropy <- function(x,base=1.01) {
  
  if(class(x)!="list") {
    stop("Please pass the vectors or tables in a list to the function.")
  }
  
  if(length(x)==1) return(-sum(c(x[[1]])*log(c(x[[1]]),base)))
  
  # figure out dimensions of each table
  
  dims <- sapply(x, function(y) {
    
    if('cpt' %in% class(y)) {
      
      length(dim(y))
      
    } else {
      
      1
      
    }
    
  })

  
  all_data_frames <- lapply(1:length(x), function(i) {
        if("cpt" %in% class(x[i][[1]])) {
          y <- attr(x[i][[1]], "model")
          y[length(y)] <- NULL
          y <- rename(y, !!paste0("wt",i) := wt)

        } else {
          y <- x[i][[1]]

        }
      y
    })
  
  # need to make a unique combination for smallest element
  biggest <- all_data_frames[dims==max(dims)][[1]]

  
  # need to create all combinations
  
  lapply(1:length(all_data_frames), function(i) {

      biggest <<- left_join(biggest, all_data_frames[i][[1]])
      
    return(NULL)
    })
  
  biggest <- biggest %>%
               rowwise %>%
                mutate(joint_pr=prod(c_across(matches('wt'))))
  
  -sum(biggest$joint_pr*log(biggest$joint_pr,base))
  
}
```

We will re-create our simply confounding graph from Figure \@ref(fig:dag1) with the `dagitty` package. This involves specifying each path in the DAG separately in a character/text variable with one path per line, and then passing it to the `dagitty` function:

```{r create_dag, echo=TRUE}

dag <- dagitty("dag { T -> Y
               Z -> T
               Z -> Y
              }")

```

We can then use the `ggdag` package to visualize the causal graph:

```{r plotdag,echo=TRUE, fig.cap="Example Visualized Directed Acyclic Graph"}
ggdag(dag)
```

To be able to calculate entropy, we need to create conditional probability tables that together make a joint probability distribution for the outcome, $Y$. The Markov property of causal graphs means we only need to consider each variable's immediate ancestors when doing so. If our outcome is $Y$, then we need to estimate the conditional distribution of $Pr(Y|T,Z)$ as these are $Y$'s two ancestors. We then need to make conditional probability tables for any of $Y$'s ancestors that have an arrow going into them, which in this case is only $T$, so we also need $Pr(T|Z)$. Any variable that does not have arrows going into it, and is connected to $Y$, needs to be included as an unconditional probability, or $Pr(Z)$. 

We can check and see which variables we need to include by identifying the ancestor variables of Y with the `parents` function from `dagitty`:

```{r ancestors, echo=TRUE}
parents(dag, "Y")
```

To create plausible conditional probability tables, we can use the `inputCPT` function from the `HydeNet` package to interactively add in probability values for each conditional probability relationship. The function takes in the names of variables using R's formula notation with the predicted variable first and any conditioning variables after the `~` sign. The user must specify probabilities for all except one level of the conditional probabilities (assuming that the last level is equal to remainder). In this case, we need two conditional probability tables:

1.  $Pr(Y | T,Z)$
2.  $Pr(T|Z)$

```{r load_joint}
joint_table1 <- readRDS("joint_table1.rds")
joint_table2 <- readRDS("joint_table2.rds")
```

I create them using the command interactively by entering in default values ("Yes" or "No") and probability values for each possible value:

```{r interact, echo=TRUE}

# commented out as it is interactive
#joint_table1 <- inputCPT(Y ~ T + Z)
#joint_table2 <- inputCPT(T ~ Z)
print(joint_table1)
print(joint_table2)
```

For $Pr(Z)$, we will have to create a data frame manually as this probability is not conditional on anything. We need to include the values of $Z$ and the probability of each outcome as the column `wt`:

```{r make_z, echo=TRUE}
joint_table3 <- tibble(Z=c("Yes","No"),
                       wt=c(0.9,0.1))
```


Given these conditional probability tables and one unconditional probability distribution, we can then quickly estimate the entropy of the causal graph by passing all of the tables and data frames as a list:

```{r est_entropy, echo=TRUE}

entropy(list(joint_table1,joint_table2,joint_table3))

```

To calculate a different causal graph given a potential study, simply create a new conditional probability table with new probability values with the `inputCPT` function.

# References

::: {#refs}
:::
