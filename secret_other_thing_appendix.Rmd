---
title: 'Appendix for Not Causal or Descriptive But Some Secret, Other Thing: Entropy as a Criterion for Causal Learning'
date: "January 11th, 2023"
bibliography: references.bib
toc: true
fontsize: 12pt
fontfamily: times
output: 
  bookdown::pdf_document2:
    keep_tex: true
    includes:
      in_header: 
        preamble2.tex
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE,
                      warning=FALSE,
                      message = FALSE,
                      fig.align = 'center')

# to install quickDAG, use *this* command to get correct version:


require(dplyr)
library(tidyr)
require(ggplot2)
require(ggthemes)
require(quickdag)
require(entropy)
require(network)
require(ggnetwork)
require(dagitty)
library(ggdag)
require(DiagrammeR)
require(rmutil)
require(jmuOutlier)
```

\newpage

# Appendix

## Tutorial for Estimating Entropy of Causal Graphs with R

In this section I show R code that can produce causal graphs, estimate conditional probability tables, and calculate entropy quickly and easily. To do so, I use four R packages: `dplyr` for data management, `HydeNet` for creating conditional probability tables, `dagitty` for defining causal graphs and `ggdag` for creating visualizations of causal graphs. Each of these packages has many more options and possibilities than I show in this brief tutorial.

```{r setup2, echo=TRUE}
library(dplyr)
library(HydeNet)
library(dagitty)
library(ggdag)
```

I further define a convenience function for calculating entropy given a list of vectors or tables of conditional probabilities. The function's arguments include a list of each conditional probability table, the list of values for each variable and the ability to set the base of the logarithm used, which defaults to 1.01.

```{r entropy, echo=TRUE}
entropy <- function(x,base=1.01) {
  
  if(class(x)!="list") {
    stop("Please pass the vectors or tables in a list to the function.")
  }
  
  if(length(x)==1) return(-sum(c(x[[1]])*log(c(x[[1]]),base)))
  
  # figure out dimensions of each table
  
  dims <- sapply(x, function(y) {
    
    if('cpt' %in% class(y)) {
      
      length(dim(y))
      
    } else {
      
      1
      
    }
    
  })

  
  all_data_frames <- lapply(1:length(x), function(i) {
        if("cpt" %in% class(x[i][[1]])) {
          y <- attr(x[i][[1]], "model")
          y[length(y)] <- NULL
          y <- rename(y, !!paste0("wt",i) := wt)

        } else {
          y <- x[i][[1]]

        }
      y
    })
  
  # need to make a unique combination for smallest element
  biggest <- all_data_frames[dims==max(dims)][[1]]

  
  # need to create all combinations
  
  lapply(1:length(all_data_frames), function(i) {

      biggest <<- left_join(biggest, all_data_frames[i][[1]])
      
    return(NULL)
    })
  
  biggest <- biggest %>%
               rowwise %>%
                mutate(joint_pr=prod(c_across(matches('wt'))))
  
  -sum(biggest$joint_pr*log(biggest$joint_pr,base))
  
}
```

We will re-create our simple confounding graph from Figure 1 in the main text with the `dagitty` package. This involves specifying each path in the DAG separately in a character/text variable with one path per line, and then passing it to the `dagitty` function:

```{r create_dag, echo=TRUE}

dag <- dagitty("dag { T -> Y
               Z -> T
               Z -> Y
              }")

```

We can then use the `ggdag` package to visualize the causal graph:

```{r plotdag,echo=TRUE, fig.cap="Example Visualized Directed Acyclic Graph"}
ggdag(dag)
```

To be able to calculate entropy, we need to create conditional probability tables that together make a joint probability distribution for the outcome, $Y$. The Markov property of causal graphs means we only need to consider each variable's immediate ancestors when doing so. If our outcome is $Y$, then we need to estimate the conditional distribution of $Pr(Y|T,Z)$ as these are $Y$'s two ancestors. We then need to make conditional probability tables for any of $Y$'s ancestors that have "open paths" to $Y$, which in this case is only $T$ due to the influence of $Z$ on $Y$ via $T$, so we also need $Pr(T|Z)$. Any variable that does not have arrows going into it, and is connected to $Y$, needs to be included as an unconditional probability, or $Pr(Z)$. As such, we multiply three terms to obtain the joint probability distribution of the causal process that produces $Y$: $Pr(Y|T,Z)Pr(T|Z)Pr(Z)$.

We can check and see which variables we need to include by identifying the ancestor variables of Y with the `parents` function from `dagitty`:

```{r ancestors, echo=TRUE}
parents(dag, "Y")
```

To create plausible conditional probability tables, we can use the `inputCPT` function from the `HydeNet` package to interactively add in probability values for each conditional probability relationship. The function takes in the names of variables using R's formula notation with the predicted variable first and any conditioning variables after the `~` sign. The user must specify probabilities for all except one level of the conditional probabilities (assuming that the last level is equal to remainder). In this case, we need two conditional probability tables:

1.  $Pr(Y | T,Z)$
2.  $Pr(T|Z)$

```{r load_joint}
joint_table1 <- readRDS("joint_table1.rds")
joint_table2 <- readRDS("joint_table2.rds")
```

I create them using the command interactively by entering in default values ("Yes" or "No") and probability values for each possible value:

```{r interact, echo=TRUE}

# commented out as it is interactive
#joint_table1 <- inputCPT(Y ~ T + Z)
#joint_table2 <- inputCPT(T ~ Z)
print(joint_table1)
print(joint_table2)
```

For $Pr(Z)$, we will have to create a data frame manually as this probability is not conditional on anything. We need to include the values of $Z$ and the probability of each outcome as the column `wt`:

```{r make_z, echo=TRUE}
joint_table3 <- tibble(Z=c("Yes","No"),
                       wt=c(0.9,0.1))
```

Given these conditional probability tables and one unconditional probability distribution, we can then quickly estimate the entropy of the causal graph by passing all of the tables and data frames as a list:

```{r est_entropy, echo=TRUE}

entropy(list(joint_table1,joint_table2,joint_table3))

```

To calculate a different causal graph given a potential study, simply create a new conditional probability table with new probability values with the `inputCPT` function.

# References

::: {#refs}
:::
