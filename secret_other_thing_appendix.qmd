---
title: 'Appendix for Not Causal or Descriptive But Some Secret, Other Thing: Entropy as a Criterion for Causal Learning'
date: today
date-format: long
author: 
  - name: Robert Kubinec
    corresponding: true
    affiliation: 
      - ref: nyuad
affiliations:
  - id: nyuad
    name: New York University Abu Dhabi
    city: Abu Dhabi
    state: Emirate of Abu Dhabi
    country: United Arab Emirates
filters:
  - line-highlight
bibliography: references.bib
toc: true
fontsize: 12pt
linestretch: 1.7
fontfamily: times
format: 
  pdf:
    documentclass: article
    keep-tex: true
    includes-in-header: 
      - file: preamble2.tex
    geometry:
      - margin=1in
    pandoc_args:
      - '--lua-filter=scholarly-metadata.lua'
      - '--lua-filter=author-info-blocks.lua'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,
                      warning=FALSE,
                      message = FALSE,
                      fig.align = 'center')

# to install quickDAG, use *this* command to get correct version:


require(dplyr)
library(tidyr)
require(ggplot2)
require(ggthemes)
require(quickdag)
require(entropy)
require(network)
require(ggnetwork)
require(dagitty)
library(ggdag)
require(DiagrammeR)
require(rmutil)
require(jmuOutlier)
```

\newpage 
<!-- 
# Discussion of Causal Inference Modes

## Counterfactuals and Manipulations

While its role in the social sciences was traditionally minor, manipulationalist and counterfactual causal inference has become the go-to reference for understanding causal relations. I consider these two paradigms, though conceptually distinct, to be grouped together as they are both fundamentally discussing the same thing. In brief, the counterfactual inference imagines an alternate world in which the causal factor is not present [@rubin1974; @holland1986], or what is known as the potential outcomes of a given unit, while the manipulationist account emphasizes human (or possibly divine) efforts to force causal factors to take on certain values [@woodward2003; @morgan2007]. These brilliant formulations are what launched the "credibility revolution" [@angrist2010; @imbens2015] and compels widespread support for RCTs across the social and biomedical sciences.

While counterfactual causal thinking has been around for some time, it has received its strongest expression in the potential outcomes literature associated with Rubin and Holland. We are told to imagine that a unit $i$ could exist simultaneously in one of two states: a treatment state $Y(1)$ in which $i$ receives a treatment and a control state $Y(0)$ where $i$ does not receive the treatment. Unfortunately, we cannot observe unit $i$ simultaneously in both states, both having and not having the treatment, which is known as the fundamental problem of causal inference. While this fundamental problem would seem to be just that, a fundamental problem, instead Rubin argued that randomly assigning units to receive the treatment $D=1$ or remain in control $D=0$ would provide an average estimate of this counterfactual difference:

$$
\hat{ATE} = E[Y(1|D=1) - Y(0|D=0)] 
$$

The reasoning is straightforward: if treatments are assigned randomly to subjects, then if we have a sufficiently large subject pool and take the average between the treatment and control groups, any one *individual* counterfactual is equally likely to occur in the treatment or control group.

It is not necessary, of course, to use randomization within a counterfactual or manipulationist theory of inference: it is simply more difficult to know whether an intervention affected the outcome. In the potential outcomes framework, the concept of ignorability determines when an observational design without randomization is able to meet the standards of a randomized design by making missing potential outcomes ignorable [@przeworski2009; @imbens2015]. By doing so, the potential outcomes framework also influenced statistics in observational research, as I describe later.

@pearl2000 significantly expanded the definition and possibilities of manipulationist/counterfactual inference by his introduction of network relations in the form of directed a-cyclic graphs (DAGs) to stand for causal relationships. Pearl's main insight is that formulations of causal relations based on conditional probabilities alone cannot capture all of what is meant by causality because variables can be associated with each other without having any direct causal relationship. By providing a very rigorous definition to the notion that correlation and causation are separate phenomena, he came up with a framework that precisely relates manipulationist ideas of inference to statistical methods of estimating relationships between variables. Pearl's approach does not need to be restricted to manipulations or counterfactuals, as I show later. I group him in with this category as he shows a preference for manipulation and counterfactual inference as higher levels of knowledge compared to association [@pearl2018, see Figure 2 in Chapter 1].

To summarize Pearl's approach to establishing causality using manipulation, imagine that there are a set of variables $Z$ and $X$ that jointly cause an outcome $Y$. To come up with a directed acyclic graph (DAG), all of the variables that are causal factors must be pointing towards the outcome $Y$ or on some path to $Y$, as in Figures 1 and 2.

```{r dag1,echo=FALSE,warning=FALSE,fig.cap='Confounded Example of a Directed Acyclic Graph',message=FALSE}
unctrl <- "X Y Z"
ctrl <- "X"

# paths
meas <- "X -> Y     
          Z -> X"
unmeas <- "Z -> Y "

output <- makeDAG(dagname='example1',filetype='png',
        text.nodes=unctrl,
        solid.edges = meas,
        dashed.edges=unmeas,
        embed=T,width = 1100, height=600)

knitr::include_graphics('example1.png',dpi=300)
```

Because all the variables are pointing in the same direction, these graphs are acyclic, or each graph can never return to its origin. A causal relation is defined explicitly as fixing one of the causal factors (the $do$ operator) so that only the manipulated variable affects the outcome, helpfully removing the confounding association between $X$ and $Z$ in Figure 1 as shown by the box around $X$ in Figure 2.

```{r dag2,echo=FALSE,warning=FALSE,fig.width=5,fig.cap='Identified Example of a Directed Acyclic Graph',message=FALSE}
unctrl <- "Z Y"
ctrl <- "X"

# paths
meas <- "X -> Y
          Z -> Y"
# call function silently
output <- makeDAG(dagname='example2',filetype='png',
        text.nodes=unctrl,
        box.nodes=ctrl,
        solid.edges = meas,
        embed=T,width=600,height=600)

knitr::include_graphics('example2.png',dpi=300)
```

This manipulation--forcing $X$ to a specific value--removes the influence of $Z$ and the "backdoor path" by which changes in $Z$ cause changes in both $X$ and $Y$. By visualizing these relationships, and providing an algorithm to convert the graph into observable probabilities, Pearl did a great service to causal inference practitioners. Yet while manipulation is certainly a core part of causality, it does not exhaust the subject.

Rather than cast aspersions on RCTs and other forms of counterfactual/manipulationist inference, it is best to think of these methods as providing a very important component of what is meant by causality. RCTs are one of the best means available for addressing unmeasurable selection attributes, such as situations where more capable students tend to select into high-performing schools. Under ideal conditions, such as with large samples, low attrition rates, and cleanly measured treatments, RCTs provide a very high amount of causal knowledge.

## The Relation Between Correlation and Causal Knowledge

Although at one time it was the dominant approach to causal inference in the sciences, the Humean conception of "constant conjunction" has fallen out of fashion. Hume supposed that we could not know why any two events occurred together and to infer any of this knowledge was a fallacy. Rather, all we could know was whether events tended to occur together. This correlational theory of inference was codified by Pearson [@pearl2018 , 53-91] and has remained a staple of statistical analysis: checking for associations between variables, or looking for risk factors, as in medicine [@boyko1990; @gershman2018].

<!-- While today's statistical education emphasizes that correlation does not equal causation, that tendency has not always been as strong among statistical practitioners. -->

<!-- The reason that correlation still matters even if we do not know the direction of causality or the mechanisms behind the correlation is because causation *does* imply association, of which correlation is a specific metric [@altman2015]. We may not be able to record the association between two variables due to confounders or selection (collider) bias, but if a causal relationship exists, then we can infer that at some level, somewhere, correlation must be happening: if $A$, then $B$. Conversely, if we can prove that two variables are perfectly uncorrelated, accounting for our knowledge of the causal process, then we have obtained good--though not perfect--evidence that a causal relationship probably does not exist. -->

<!--# For this reason, observational analysis does have a close relationship to causality even if it does not define it . When we observe a correlation among human-generated outcomes, we know that a causal process is likely at work, although we may not know which set of factors are the most important (i.e., causal identification). Statistical methods only give us a way to quantify the uncertainty in estimating the correlation, not in determining whether the association is "truly" causal. But if we abandon the notion that there is a gold standard for causality, then observational analysis, or searching for correlations, still matters for generating casual knowledge even if we cannot learn everything we want to learn.  

While some are willing to dismiss traditional statistical methods without randomization or at least manipulation of treatments, entire fields of science are based on purely observational analysis, including astronomy, forensic anthropology, paleontology, and so on. We have never manipulated giant bodies of gas to force them to explode, but we are still fairly sure we know what supernova are [@bethe1990]. The reason for this is that as a silver standard, observational analysis does provide causal knowledge if ideal conditions are met, as is true of randomized experiments. In particular, observational methods provide evidence of--though never fully determine--causal relations when we know which variables are relevant to the outcome and in what way, the data provide accurate measures of these variables, and we have as much data as we might want.

Researchers also often use syllogisms to interrogate models [@ashworth2021] and reason their way through situations where they cannot collect all the data they want, such as astronomers' disputes over the location and number of planets. If an orbit reflects certain instabilities, then a planet can be hypothesized to exist even if there is no direct evidence for it [@smith1989].

The re-orientation away from observation as a form of causal inference has led to the rise of the so-called quasi-experimental research designs. These methods, including difference-in-difference and regression discontinuity designs, are confusing to define because they make reference to experimental treatments but treatment assignment is never manipulated by the researcher. Instead, these models' superiority over other observational methods is that they can be expressed in Rubin's potential outcomes notation in a way that establishes ignorability [@lee2008; @abadie2005]. However, establishing the conditions of ignorability does not itself make those conditions more likely to be met, and practitioners seem to expect that these models will work "out of the box" better than other models.

Aside from the clear expression of the ignorability assumptions, such as parallel trends, these models are fundamentally derivations of well-known statistical models. Difference-in-differences is an interactive fixed effects model with panel data [@kropko2020]. Regression discontinuity design is a form of non-parametric regression where the predictor is evaluated at a single point [@lee2008; @calonico2014]. Causal identification in these models requires knowing a fair amount of detail about the true data-generating process (i.e., the causal graph), leading some to argue that these methods have become over-employed and poorly understood [@kahn-lang2018; @caughey2011; @grimmer2011]. To proxy for random assignment, scholars have come to emphasize statistical tests for incomparability between units, such as pre-treatment trends, but these statistical fixes can have the unfortunate side effect of encouraging type II errors given the low power of these tests [@roth2021; @angrist2019].

These models are certainly useful, but are best understood within the general framework of observational analyses that can provide causal knowledge given the right conditions rather than clever loopholes that allow a scholar to claim causality without an experiment. It is entirely possible to obtain causal knowledge from observational methods without having to resort to a quasi-experimental design. One oft-cited example of the ideal conditions for observational inference comes from the analysis of the correlation between smoking and lung cancer [@cornfield1959]. Another, although it is not often expressed this way, is the application of polling aggregation models to predict electoral outcomes [@campbell2016]. While election forecasting models are not perfect and sometimes over or under-predict, the underlying causal process is usually undisputed: if a person is asked who they will vote for the day before the election, they will very likely cast a vote for that person in the ballot box. In this situation, there is plenty of data, we can measure most of what we want to know, and the measures are fairly direct of the underlying outcome.

## Mechanistic Causation

While the previous two approaches are often contrasted as the only ways of thinking about causality in the sciences, there is a third philosophy that is gaining traction among qualitative researchers, especially in sociology and political science. There has been much work in these disciplines in recent years on establishing how case study research can identify mechanisms that link causal variables [@bennett2014; @abbott1992; @evera1997]. Again, instead of considering this line of inquiry to be a subsidiary issue to the question of causality, I propose that mechanisms are a part of what we mean by causality, and hence are their own silver standard. When ideal conditions are met, we can infer causality with reasonable, though never perfect, confidence.

Several definitions of mechanisms have been proposed in the literature [@mahoney2012; @gerring2017]. I use for this paper the standard of @waldner2015 that mechanisms are invariant processes connecting causal variables to each other. The example he uses is very instructive: the person who discovered the mechanisms through which aspirin relieved heart pressure, Sir John Vane, received a Nobel Prize in 1982, despite the fact that it was already known that aspirin had beneficial properties. This Nobel prize is puzzling under either the observational or experimental approaches to causality: if the strength of association was indisputable and random assignment had provided an estimate of all possible counterfactuals, then what questions remained for Sir John Vane to receive a Nobel for?

The answer is that when humans conceive of causality, we imagine there to be some kind of process linking cause and effect. Defining exactly what this is process is can be difficult, but the invariant standard is a helpful step. We are looking for processes that are so low-level that they operate similarly in all contexts. In a social scientific setting, we might think of basic emotions like fear, anger and happiness [@pearlman2013], or the rational actor model [@elster1994].

Waldner's theory of causation is particularly useful in this paper as it directly compares the mechanistic thinking of causation to Pearl's use of network diagrams for causal structures. To integrate mechanisms, we can introduce labels on the edges that identify which mechanism is in operation.[^1] Importantly, *these mechanisms are not variables*, as causal mediation analysis pre-supposes [@imai2010]. Rather, mechanisms are root processes that are at the limit of our observational capacity and are so fundamental as to be nearly determinate to the outcome. Causal mediation assumes we can collect ample data on the mediators, but if we are at the limit of our observational capacity, the scope of data collection is necessarily limited and the necessary quantification may result in important loss of information that is difficult to capture in a rectangular dataset [@collier2010]. It is of course always possible to improve the quantified measurement of mechanisms to the point that they can be treated as a mediators, which is a point of connection between mechanistic and other modes of inference.

[^1]: While @waldner2015 first proposed that edges could be labeled as representing mechanisms, Pearl has also come to the same interpretation. See @cinelli2019, footnote 2.

Like the other two silver standards, under ideal conditions mechanistic research can provide strong evidence of causality. In the social sciences this entails collecting exhaustive evidence on a person's decision-making in what has come to be called process-tracing. Process-tracing methodologists refer to the idea of a "smoking gun" as the kind of evidence that establishes causality within this framework, such as obtaining a private diary of an important leader that describes in detail why they made their decisions [@evera1997; @collier2010; @bennett2014; @humphreys2015]. These ideal conditions are relatively rare, but like experimental and observational approaches, we can have confidence in inferring a high level of causal knowledge when we have all the information we might want about how $X$ changed into $Y$.

<!--# For example, imagine if we wanted to know whether the U.S. bombing of Dresden influenced Hitler's strategies during World War II. In a qualitative framework, we could obtain a smoking gun if we had a private diary by Hitler's own hand that described verbatim his thoughts on the U.S. bombing of Dresden and whether or not he shifted war tactics in response to the bombing. If we then also had detailed evidence, such as transcripts of meetings and official orders, carrying out these changes in tactics prescribed in the journals, then we could state with a high degree of confidence that we know that the bombing of Dresden had a causal impact on Hitler's decision-making. 

Of course, qualitative inference does lend itself to single-act causation as opposed to establishing general trends between variables. However, to the extent that the variables under study are the same across conditions, then we could presume that our knowledge of the mechanisms linking these variables is similarly invariant. If we can establish what the mechanisms are then we can be more confident that the variables are causally associated in addition to any relationship we estimate using observational or experimental statistics.

## Synthesis

> "Of course it is happening inside your head, Harry, but why on earth should that mean that it is not real?" - Professor Albus Dumbledore from J.K. Rowling's *Harry Potter and the Deathly Hallows*

The point of presenting each of these ways of inferring causality is to put forward a more liberal conceptualization of causal inference for the social sciences. To paraphrase the U.S. Supreme Court Justice Potter Stewart, we may not be able to define causality exhaustively, but we know it when we see it. In an exhaustive summary, @sloman2015 argue based on extensive experimental evidence that while the network perspective of Pearl solved many issues in formalizing causal thinking, it still "has not offered a silver bullet that answers all questions about human thought" (p. 240). What causality exactly means is a subject for debate, even if we have made remarkable strides in terms of precisely detailing certain causal logics in the past three decades.

In common parlance in the social sciences, a "causal inference" model implies either a randomized control trial or the usage of certain statistical methods that can be expressed using Rubin's potential outcomes notation. This slippage in terminology puts the cart before the horse: causality does not inherit from counterfactuals; rather, counterfactuals arose as a way of expressing what is meant by causality. No one framework has yet to define all of what we mean by causality, and if we do so, such as by claiming that RCTs are a "gold standard", we will inevitably end up deflating the value of other, silver, standards.

However, it may yet be possible to construct a more satisfying, over-arching framework. @spirling2022's advocacy for the "inference to the best explanation" criterion is a compelling example of how we might be able to more directly integrate different research traditions into a single inference method. As such, I do not think it is necessary to adopt an approach like "causal pluralism" that does not want to define standards for causal inference [@reiss2009]. Rather, the aim is for a unitary conception of causation that can still allow for diversity in research methods [@gerring2006]. The aim of this paper is not to propose my own framework, but to allow the inference modes to exist even in their apparent contradictions, and use entropy to adjudicate the amount of causal learning each contains, as I demonstrate in the next section.

Nonetheless, causality will remain a latent concept: we cannot observe it, and our uncertainty over the term will never completely go away. Permitting this remaining uncertainty, and allowing the use of the word "causal" in more settings than just randomized control trials or potential outcome models, will help avoid mis-perceptions and researcher frustration. Ultimately, by replacing the descriptive/causal dichotomization with an emphasis on the relative amount of causal knowledge obtained from a study, we can incorporate this uncertainty while still upholding strict standards for inference.

-->
# Case Study: Oil Wealth and Democratization

One of the most actively-research questions in comparative political science concerns the theorized relationship between a country discovering oil resources and subsequent resistance to democratization. Known as the rentier state theory [@mahdavyhossein1970; @therent1987], this influential hypothesis has received both significant support [@ross2001; @ross2012] and resistance from scholars [@haber2011; @cherif2015]. Of particular relevance for the application of causal entropy is the recent work of @waldner2021, who review the literature and produce a compelling causal graph that is able to summarize much of the debate while also making their new claims about the underlying causal structure.

I reproduce their article's Figure 5 in @fig-resourcedag where variable $A$ stands for "autocratic resilience", $B$ for "British policy", $P$ for "protection", $O$ for "oil" and $S$ for "survival." Waldner and Smith's argument is that there is no actual causal arrow between oil and autocratic resilience. Rather, there appears to be such a relationship because of what is known as collider bias (or selection bias) caused by variable $P$. Waldner and Smith make the case that British policy caused autocratic resilience among the heavily authoritarian Gulf states, and oil is associated with authoritarian institutions because oil endowments helped these states secure protection from foreign rivals such as Saudi Arabia and Iran. As a result, these conservative monarchies were more likely to survive and more likely to be authoritarian, inducing a spurious association between oil resources and authoritarian institutions.

```{r resourcedag,fig.cap="Replication of Figure 5 from Waldner and Smith (2020)"}
#| label: fig-resourcedag

resource_dag <- "dag {
                      O -> P
                      P -> S
                      B -> P
                      B -> A
                      }"

rd <- dagitty(resource_dag)
ggdag(rd) + theme_tufte() +
  labs(x="",y="") +
  theme(text=element_text(family=""),
        axis.text = element_blank())
```

The aim of this case study is not to adjudicate this original claim but rather to use their causal graph as a basis for evaluating possible research studies using causal entropy. This causal graph is far more involved than the previous one, and as a result, the conditional probability tables are more complex. To simplify matters, we will focus on the probability of authoritarian resilience ($A$) conditional on British policy ($B$) and survival $S \in \{\text{High},\text{Low}\}$:

|                              | $A = \text{Endure}$ | $A = \text{Democratize}$ |
|----------------------------|--------------------|------------------------|
| $B = \text{Intervention}$    | 0.5                 | 0.5                      |
| $B = \text{No Intervention}$ | 0.5                 | 0.5                      |

: $Pr(A | B , S = \text{Low})$

|                              | $A = \text{Endure}$ | $A = \text{Democratize}$ |
|----------------------------|--------------------|------------------------|
| $B = \text{Intervention}$    | 0.5                 | 0.5                      |
| $B = \text{No Intervention}$ | 0.5                 | 0.5                      |

: $Pr(A | B , S = \text{High})$

As can be seen in both tables, given that this is a new argument, we will use as our prior distribution complete uncertainty. Otherwise, we would be biasing our results in favor of Waldner and Smith's contention. Furthermore, there is an additional problem with arriving at a more informative prior graph: we cannot manipulate these variables, and we also cannot observe variation naturally. The Gulf states have always been sovereign since their independence, and British policy in question occurred between the 1920s and 1950s as the monarchies consolidated their rule. Because we cannot go back in time to collect more data, we are much more limited in what tools we can use to arrive at more informative distributions for our causal graph.

```{r calc_ent_authorit}

prob_out_post <- crossing(A=c("Endure","Democratize"),
                          B=c("Intervention","No Intervention"),
                          S=c("Low","High"),
                          Omega=c("Interventionist","Non-Interventionist")) %>% 
  mutate(pr_joint=case_when(Omega=="Interventionist" ~ .5^3 * .9,
                            TRUE ~ .5^3 * .1))

prob_out_null <- crossing(A=c("Endure","Democratize"),
                          B=c("Intervention","No Intervention"),
                          S=c("Low","High"),
                          Omega=c("Interventionist","Non-Interventionist")) %>% 
  mutate(pr_joint=.5^4)

ent_orig <- -sum(prob_out_null$pr_joint*log(prob_out_null$pr_joint,1.01))
ent_post <- -sum(prob_out_post$pr_joint*log(prob_out_post$pr_joint,1.01))
```

As a result, the mechanisms are very important. Waldner and Smith present evidence on a crucial mechanism--the nature of British intervention in Gulf politics. In other words, what were the intentions of British policymakers towards these states? Did they have an explicit policy of promoting authoritarianism? We can label this mechanism $\Omega = \{\text{Interventionist},\text{Non-Interventionist}\}$, and then consider the joint distribution $Pr(A,S | \Omega) Pr(\Omega | B) Pr(B)$. While evaluating the quality of Waldner and Smith's historical work is beyond the scope of this short study, we will give them the benefit of the doubt and say that they can establish the $Pr(\Omega | B = \text{Interventionst} = 0.9)$. We can then calculate the reduction of log entropy for $Pr(A,S | \Omega) Pr(\Omega | B) Pr(B)$ vis-a-vis a null mechanism of $Pr(\Omega = \text{Interventionst} | B) = 0.5)$, which leads to a reduction in entropy from `r round(ent_orig,0)` to `r round(ent_post,0)`. As a result, their process-tracing reduces our uncertainty considerably, although learning about only one mechanism still leaves substantial uncertainty.

Another way we can learn about this causal graph is to focus on the hypothesized null relationships. The graph in @fig-resourcedag has no direct connection between oil $O$ and authoritarian resilience $A$. There have been experimental and quasi-experimental studies that have studied a hypothesized mediator known as demands for accountability in which higher reliance of the state on rents will cause reduced demands for accountability. We can then update our causal graph with this hypothesized relation in @fig-resourcedag2 by denoting demand for accountability as $D$.

```{r resourcedag2,fig.cap="Causal Diagram with Hypothesized Mediator $D$"}
#| label: fig-resourcedag2

resource_dag2 <- "dag {
                      O -> P
                      P -> S
                      B -> P
                      B -> A
                      O -> D
                      D -> A}"

rd2 <- dagitty(resource_dag2)
ggdag(rd2) + theme_tufte() +
  labs(x="",y="") +
  theme(text=element_text(family=""),
        axis.text = element_blank())
```

We now have an open causal pathway between oil resources $O$ and authoritarian resilience $A$. However, because we still have the collider relationship between protection $P$ and survival $S$, we should be worried about studies that do not take into account this possibility for selection bias. Experimental inference would seem to be a good option here, although we are limited in what we can focus on in the graph. We could either try to manipulate oil resources (or find plausibly exogenous increases in natural resources) and see whether demands for accountability change (path $O \rightarrow A$), or we could look at whether increases in demands for accountability lead to lower authoritarian resilience (path $D \rightarrow A$).

Neither is particularly easy to achieve, though recent work has shown it can be done. @paler2013 employed an experimental treatment to prime Indonesians into thinking either about taxes or "windfall" natural resources as a proportion of their local government's budget. She found a 5-6 pp increase in accountability actions by survey respondents in the taxation versus windfall condition. On the other hand, @delacuesta2019 deployed a lab-in-the-field experiment with a similar informational treatment in Ghana and Uganda but found little difference between taxation and other sources of revenue when it came to demands for accountability.

Estimating the $D \rightarrow A$ path may be even harder as it requires measures of authoritarian persistence. @kubinec deployed an interactive informational treatment during Algeria's national protest movement in 2019 and found that informing Algerians about the extent of government redistribution had strongly varying effects by respondent wealth. More wealthy respondents were less willing to join protests opposing the regime, while poorer respondents became more likely to do so following the treatment. In sum, the experimental evidence appears to be mixed on this question, possibly justifying Waldner and Smith's lack of a causal arrow from $O \rightarrow A$.

Finally, we could also consider studies that are observational in nature, such as @ross2001 and @haber2011. From the causal graph, it should be clear that these studies will be unable to be causally-identified because it is not possible to adjust for selection bias with "control" variables in regression [@hünermund], and a selection model would require at least some observations of countries that did not survive [@waldner2021, p. 898]. As such, we would have to interpret observational studies that measure the association between natural resources and authoritarianism as assessing the *joint* distribution $Pr(A,P,S,B|O)Pr(O)$--in other words, we can only factor out oil resources. One advantage is that oil resources have considerable variation, but a disadvantage is that oil resources change little over time. As such, our learning from these studies predominantly depends on how much oil varies vis-a-vis the joint distribution, and in any case, we will not be able to isolate the effect of oil on authoritarianism apart from the proposed selection process.

As such, in this case study it would appear that experimental approaches and qualitative or mechanistic studies would be most aptly suited to research progress--unless observational work was able to better model the possible selection processes. Simply observing oil resources is unlikely to offer much type I causal learning without an ability to adjust for other important variables.

# Tutorial for Estimating Entropy of Causal Graphs with R

In this section I show R code that can produce causal graphs, estimate conditional probability tables, and calculate entropy quickly and easily. To do so, I use four R packages: `dplyr` for data management, `HydeNet` for creating conditional probability tables, `dagitty` for defining causal graphs and `ggdag` for creating visualizations of causal graphs. Each of these packages has many more options and possibilities than I show in this brief tutorial.

```{r setup2, echo=TRUE}
library(dplyr)
library(HydeNet)
library(dagitty)
library(ggdag)
```

I further define a convenience function for calculating entropy given a list of vectors or tables of conditional probabilities. The function's arguments include a list of each conditional probability table, the list of values for each variable and the ability to set the base of the logarithm used, which defaults to 1.01.

```{r entropy, echo=TRUE}
entropy <- function(x,base=1.01) {
  
  if(class(x)!="list") {
    stop("Please pass the vectors or tables in a list to the function.")
  }
  
  if(length(x)==1) return(-sum(c(x[[1]])*log(c(x[[1]]),base)))
  
  # figure out dimensions of each table
  
  dims <- sapply(x, function(y) {
    
    if('cpt' %in% class(y)) {
      
      length(dim(y))
      
    } else {
      
      1
      
    }
    
  })

  
  all_data_frames <- lapply(1:length(x), function(i) {
        if("cpt" %in% class(x[i][[1]])) {
          y <- attr(x[i][[1]], "model")
          y[length(y)] <- NULL
          y <- rename(y, !!paste0("wt",i) := wt)

        } else {
          y <- x[i][[1]]

        }
      y
    })
  
  # need to make a unique combination for smallest element
  biggest <- all_data_frames[dims==max(dims)][[1]]

  
  # need to create all combinations
  
  lapply(1:length(all_data_frames), function(i) {

      biggest <<- left_join(biggest, all_data_frames[i][[1]])
      
    return(NULL)
    })
  
  biggest <- biggest %>%
               rowwise %>%
                mutate(joint_pr=prod(c_across(matches('wt'))))
  
  -sum(biggest$joint_pr*log(biggest$joint_pr,base))
  
}
```

We will re-create our simple confounding graph from Figure 1 in the main text with the `dagitty` package. This involves specifying each path in the DAG separately in a character/text variable with one path per line, and then passing it to the `dagitty` function:

```{r create_dag, echo=TRUE}

dag <- dagitty("dag { T -> Y
               Z -> T
               Z -> Y
              }")

```

We can then use the `ggdag` package to visualize the causal graph:

```{r plotdag,echo=TRUE, fig.cap="Example Visualized Directed Acyclic Graph"}
ggdag(dag)
```

To be able to calculate entropy, we need to create conditional probability tables that together make a joint probability distribution for the outcome, $Y$. The Markov property of causal graphs means we only need to consider each variable's immediate ancestors when doing so. If our outcome is $Y$, then we need to estimate the conditional distribution of $Pr(Y|T,Z)$ as these are $Y$'s two ancestors. We then need to make conditional probability tables for any of $Y$'s ancestors that have "open paths" to $Y$, which in this case is only $T$ due to the influence of $Z$ on $Y$ via $T$, so we also need $Pr(T|Z)$. Any variable that does not have arrows going into it, and is connected to $Y$, needs to be included as an unconditional probability, or $Pr(Z)$. As such, we multiply three terms to obtain the joint probability distribution of the causal process that produces $Y$: $Pr(Y|T,Z)Pr(T|Z)Pr(Z)$.

We can check and see which variables we need to include by identifying the ancestor variables of Y with the `parents` function from `dagitty`:

```{r ancestors, echo=TRUE}
parents(dag, "Y")
```

To create plausible conditional probability tables, we can use the `inputCPT` function from the `HydeNet` package to interactively add in probability values for each conditional probability relationship. The function takes in the names of variables using R's formula notation with the predicted variable first and any conditioning variables after the `~` sign. The user must specify probabilities for all except one level of the conditional probabilities (assuming that the last level is equal to remainder). In this case, we need two conditional probability tables:

1.  $Pr(Y | T,Z)$
2.  $Pr(T|Z)$

```{r load_joint}
joint_table1 <- readRDS("joint_table1.rds")
joint_table2 <- readRDS("joint_table2.rds")
```

I create them using the command interactively by entering in default values ("Yes" or "No") and probability values for each possible value:

```{r interact, echo=TRUE}

# commented out as it is interactive
#joint_table1 <- inputCPT(Y ~ T + Z)
#joint_table2 <- inputCPT(T ~ Z)
print(joint_table1)
print(joint_table2)
```

For $Pr(Z)$, we will have to create a data frame manually as this probability is not conditional on anything. We need to include the values of $Z$ and the probability of each outcome as the column `wt`:

```{r make_z, echo=TRUE}
joint_table3 <- tibble(Z=c("Yes","No"),
                       wt=c(0.9,0.1))
```

Given these conditional probability tables and one unconditional probability distribution, we can then quickly estimate the entropy of the causal graph by passing all of the tables and data frames as a list:

```{r est_entropy, echo=TRUE}

entropy(list(joint_table1,joint_table2,joint_table3))

```

To calculate a different causal graph given a potential study, simply create a new conditional probability table with new probability values with the `inputCPT` function.

## Using CausalQueries to Calculate Entropy with Uncertainty

The R package `CausalQueries`, which is the package the implements the research design methods proposed in @humphreys2023, permits novel types of inferences over causal graphs as defined in the previous section. `CausalQueries` allows the user to specify both point estimates for the probability of edge relations in a causal graph, or what @humphreys2023 call "parameters", along with uncertainty in those parameters through the use of Dirichlet priors over the values of different node-to-node relationships. With this uncertainty, it is possible to talk about a distribution of entropies from a given research design/causal graph rather than just a point estimate. Helpfully, `CausalQueries` is also compatible with the `dagitty` package for specifying DAGs, and so we can employ the same example above.

We will first use the function `make_model()` to create a `CausalQueries` object given our DAG specification:

```{r}

library(CausalQueries)

dag_cq <- make_model("{ T -> Y
                      Z -> T
                      Z -> Y }")

plot(dag_cq)

```

By default, `CausalQueries` creates null graphs or maximum entropy graphs with uniform priors over edge relations. The function below contributed by Macartan Humphreys shows how to calculate entropy from a `CausalQueries` object given its value of parameters and priors:

```{r calc_entropy_null}

get_entropy <- function(model, parameters = NULL,base=1.01) {
  p <- get_event_prob(model, parameters)
  -sum(p*log(p,base=base))
}

get_entropy(dag_cq)

```

This value matches the value of the maximum entropy or null graph described in the paper in the case study of the COVID-19 vaccine.

To discuss the entropy of plausible interventions, we need to change the parameters or priors of this object to reflect our beliefs about the relationships between nodes in the causal graph. Doing so is a more involved process than specifying the conditional probability tables as in the previous section as `CausalQueries` can be used for a range of tasks beyond entropy calculation, and as such requires more information about the graph. However, specifying such a graph permits the usage of a range of powerful `CausalQueries` features, including the role of plausible omitted confounders and, as we show, obtaining uncertainty in the estimates of entropy.

`CausalQueries` uses a syntax in which each node can take on binary values (0 or 1) and then allows the user to specify the probability of inter-node relationships. This can result in quite a large number of probabilities. For the three-node causal graph above, we end up with needing to specify 16 different probabilities for the terminal node $Y$ because $Y$ can take on values for each value of the preceding nodes $Z$ and $T$ as in the code below. The `set_parameters` function allows the user to specify these probabilities for each node outcome, which follows a syntax described in their [package working paper](https://macartan.github.io/assets/pdf/papers/2024_CausalQueries.pdf). In brief, a nodal value of 0000 for $Y$ corresponds to the probability that is $Y=0$ for all possible combinations of $Z$ and $T$. Since $Z$ and $T$ each have 2 values, this corresponds to four separate probabilities that must be multiplied together-`r .02*.15*.98*.85`-as in the code below.

```{r true_graph}

cq_true <- set_parameters(dag_cq,
                          parameters=c(.25,.75),
                          node="Z",nodal_type=c("0","1")) %>% 
          set_parameters(parameters=c(.9*.1,.1*.1,.9*.9,.1*.9),
                         node="T",
                         nodal_type=c("00","10","01","11")) %>% 
  set_parameters(parameters=c(.02*.15*.98*.85, # 0000
                              .98*.15*.98*.85, # 1000
                              .02*.85*.98*.85, # 0100
                              .98*.85*.98*.85, # 1100
                              .02*.15*.02*.85, # 0010
                              .98*.15*.02*.85, # 1010
                              .02*.85*.02*.85, # 0110
                              .98*.85*.02*.85, # 1110
                              .02*.15*.98*.15, # 0001
                              .98*.15*.98*.15, # 1001
                              .02*.85*.98*.15, # 0101
                              .98*.85*.98*.15, # 1101
                              .02*.15*.02*.15, # 0011
                              .98*.15*.02*.15, # 1011
                              .02*.85*.02*.15, # 0111
                              .98*.85*.02*.15), # 1111
                 node="Y",
                 nodal_type=c("0000","1000",
                              "0100","1100",
                              "0010","1010",
                              "0110","1110",
                              "0001","1001",
                              "0101","1101",
                              "0011","1011",
                              "0111","1111"))

```

To find these probabilities, they can be identified from the conditional probability tables listed above. The probability that $Y=0$ when $Z=0$ and $T=0$, for example, is equal to the probability that $T$=No Vaccine and $I$=Not Infected in Table 2 in the main text (i.e., when $Z$=Young or 0). This value of .02 is equal to the first of the four nodal values of $Y$ for the first row 0000, i.e., the first element of that row is equal to the probability of $Y$ for that particular combination of values for $Z$ and $T$. To learn what those nodal types signify, you can access the formula from a `CausalQueries` object with this function:

```{r causal_types}

get_nodal_types(cq_true)

```

As can be seen, the first element in the nodal types of $Y$ (i.e. `Y[*]***`) is equal to the probability of $Y$ when $Z=0$ and $T=0$. So the user has to match of these four outcomes of $Y$ given $Z$ and $T$ from the conditional probability tables to specific cells in the conditional probability tables. While this involves specifying a significant number of values for $Y$, there is also much repetition in the formula that simplifies the task. For example, the difference between the nodal types $Y=0000$ and $Y=1000$ is only one element, or the probability of $Y$ when $Z=0$ and $T=0$. The user only needs to change the code `r `.02*.15*.98*.85` to `r .98*.15*.98*.85`, that is, change the probability of $Y=0$ to $Y=1$: 1 - 0.02 = 0.98.

However, the payoff to specifying the probabilities in this way is to access all of the functions of `CausalQueries` as previously mentioned. While we have changed the `parameters` of the `CausalQueries` object, we can also look at the `priors`:

```{r causalpriors}
get_priors(cq_true)
```
The priors in a `CausalQueries` object are Dirichlet priors over nodal types in which each nodal type is given a partition of the total probability. The prior number, called `alpha`, for a nodal type can be any positive number, and if all the of the `alpha`s are the same for a node, such as $Z.0=1$ and $Z.1=1$ as in the code above, then both outcomes have equal probability. This would correspond to a maximum entropy distribution for $Z$ (uniform distribution). Note that these priors are separate from the parameters (point estimates) that we specified previously. 

To add more information into these priors, we would want to increase the value for specific nodes. These can be given the interpretation of "prior counts of successes" or what can be more easily understood as "prior sample size". Suppose we form our priors based on 20 independent observations for the probability of $Z$ and based on these observations we believe both outcomes of $Z$ to be equally probable. In that case, we would update the priors to be equal to 10 for both values of $Z$:

```{r setprior1}
cq_true <- set_priors(cq_true,alphas=c(10,10),node="Z")
get_priors(cq_true)
```

We now believe that both outcomes of $Z$ are equally plausible, but we are more certain that they are equally plausible than the default prior for which we had only two observations. If from our prior sample size we believe that 15 observations supported $Z=1$ and 5 observations supported $Z=0$, we could use that prior specification for $Z$:

```{r}
cq_true <- set_priors(cq_true,alphas=c(5,15),node="Z")
get_priors(cq_true)
```
It can of course be hard to know exactly what to set these priors to. These priors could be set based on relative heuristics, i.e., use a prior sample size of 50 for $Z$, 30 for $T$ and 100 for $Y$. An alternative strategy is to sample from the parameters we set earlier and then see what a certain amount of data would imply about the posterior distribution of these nodal types. It is easy to sample from a `CausalQueries` object:

```{r sample_queries}
sample_data1 <- make_data(cq_true,n=100)
table(select(sample_data1,Y,T,Z))
```
We can see that we have data generated for our two cases for we have a young ($Z=0$) and old ($Z=1$) population. For young people, the vaccine is very effective (0 people who get the vaccine also get COVID-19 (T=1)) while it is less effective among the elderly (10 people get COVID-19 who get the vaccine). With this data, we can see how our the priors update as well, which is called updating the model in `CausalQueries`:

```{r}
cq_true_update1 <- update_model(cq_true,data=sample_data1,refresh=0,
                               keep_fit=TRUE,keep_transformed=TRUE,
                               keep_event_probabilities = TRUE)
head(cq_true_update1$posterior_distribution)
```

These are the posterior estimates for our nodal values, with each a plausible draw from the Dirichlet distribution over nodes. We can visualize the uncertainty via histograms:

```{r}
hist(cq_true_update1$posterior_distribution$Z.1)
```

We see that we expect there to be about 70 - 75% old people in the distribution with a confidence interval roughly 62 to 82 percent. 

Utilizing this feature of `CausalQueries` is quite useful as it allows us to understand that, given a set research design and parameter values that we can stand to learn, what is our residual uncertainty about the causal relationships for varying amounts of data? For example, we can repeat the above analysis but generate 1000 independent samples from the true causal graph:

```{r update2}
sample_data2 <- make_data(cq_true,n=1000)
cq_true_update2 <- update_model(cq_true,data=sample_data2,refresh=0,
                               keep_fit=TRUE,keep_transformed=TRUE,
                               keep_event_probabilities = TRUE)
hist(cq_true_update2$posterior_distribution$Z.1)
```

We can see now that the average value is the same but our uncertainty has shrunk.

Given these posterior draws, we can also calculate differences in our uncertainty in entropy. Below I update both distributions with our two samples (100 and 1,000 observations) and then plot the resulting distributions in entropy:

```{r diff_ent}

# need a new function for calculating the entropy of a distribution of probabilities

ent_func <- function(p,base) -sum(p*log(p,base=base))

entropy_dist <- function(mod, base=1.01) {
  
  apply(mod$stan_objects$w, 1, ent_func, base=base)
  
}
 

print("Entropy of 100 observations:")
summary(entropy_dist(cq_true_update1))
print("Entropy of 1,000 observations:")
summary(entropy_dist(cq_true_update2))
```
The average entropies are not identical due to sampling variation. However, it is still easy to see that the 100 observations model has a much wider spread in entropy than the 1,000 observations model. This meta-uncertainty can be helpful in making inferences over plausible research designs more robust. 


# References

::: {#refs}
:::
